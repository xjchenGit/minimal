<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Xuanjun Chen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">

    <style>
      .wrapper { display: flex; max-width: 1100px; margin: 0 auto; position: relative; }
      
      header { 
        width: 280px; 
        height: 100vh; 
        position: sticky; 
        top: 0; 
        padding: 40px 0;
        border-right: 1.5px solid;
        border-image: linear-gradient(to bottom, rgba(238,238,238,0) 0%, rgba(238,238,238,1) 15%, rgba(238,238,238,1) 85%, rgba(238,238,238,0) 100%) 1 100%;
      }
    
      section { flex: 1; padding: 40px 50px; }
    
      /* --- æ‰‹æ©Ÿç‰ˆ RWD æŽ§åˆ¶ --- */
      @media (max-width: 768px) {
        .wrapper { display: flex; flex-direction: column; width: 100% !important; padding: 0 !important; }
        
        header {
          width: 100% !important;
          height: auto !important;
          position: sticky !important;
          top: 0;
          z-index: 1000;
          background: #fff;
          padding: 10px 0 !important;
          border-right: none !important;
          border-bottom: 1px solid #eee !important;
          box-sizing: border-box !important;
        }
        
        section {
          width: 100% !important;
          padding: 20px !important;
          box-sizing: border-box !important;
          display: flex; 
          flex-direction: column;
        }

        footer.site-footer {
          order: 99;
          margin-top: 40px;
          padding-top: 20px;
          border-top: 1px solid #eee;
          text-align: center;
          width: 100%;
        }
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header id="sidebar"></header>
		
      <section>
	  	<!-- <nav style="margin-bottom: 30px; font-family: 'Source Serif 4', serif; text-align: right;">
	        <a href="index.html" style="text-decoration: none; color: #3a7bd5; font-weight: 600; margin-right: 15px;">About</a>
	        <a href="publications.html" style="text-decoration: none; color: #666; font-weight: 600;">Publications</a>
	    </nav> -->
        <h2>About me</h2>

        <p align="justify"> I am a Ph.D. student at <a href="https://www.ntu.edu.tw/english/" target="_blank">National Taiwan University (NTU)</a>, advised by Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" target="_blank">Hung-yi Lee</a> and Prof. <a href="http://mirlab.org/jang/cv/cv.asp" target="_blank">Jyh-Shing Roger Jang</a>. 
			I received M.S. degree from <a href="https://www.ntu.edu.tw/english/" target="_blank">NTU</a> in 2023 and B.S. degree from <a href="https://www.ntust.edu.tw/index.php?Lang=en" target="_blank"> Taiwan Tech </a> in 2020. My research interests include, Speech Processing, Audio/Text LLMs, and Audio Deepfakes. </p>
			<!-- I'm honored to receive <b>Google Student Travel Grant</b> in 2024. </p> -->
	  	<p align="justify"> ðŸ’¡ I am a newcomer to the field of Audio/Text Large Language Models. Prior to this, I spent over 5 years working on Audio Deepfake problems. <p>
			<!-- <br> -->
			
		<!-- <h2>Research Interests</h2>
	<p align="justify">My <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> is here! My research interests span deep learning, audio-visual learning, and speech processing. We focus on deepfake detection from a defenderâ€™s perspective, developing methods to counter specific threats like speech synthesis [1], singing voice synthesis [9], and adversarial attacks [2, 8]. Additionally, weâ€™re creating a lightweight audio-visual synchronization model to identify multi-modal out-of-sync [10] and have developed a dataset [4] incorporating recent advancements in speech synthesis to enhance defense against synthetic voice threats.</p> -->
	<!-- <h2>Research Interests</h2>
	<p align="justify">
		Here is my <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> and research experience:
	<br>
	<strong>1) Multimodal Deepfake Detection</strong>
	   <ul align="justify">
	      <li>Modeling: [<a href="https://arxiv.org/abs/2406.03111"><u>Interspeech '24</u></a>], [<a href="https://arxiv.org/abs/2406.04582"><u>Interspeech '24</u></a>]</li> -->
		<!-- <li>Modeling: <u>SingGraph</u> [<a href="https://arxiv.org/abs/2406.03111">Interspeech '24</a>], [<a href="https://arxiv.org/abs/2406.04582">Interspeech '24</a>]</li> -->
		<!-- <li>Efficiency: [<a href="https://ieeexplore.ieee.org/abstract/document/10446372"><u>ICASSP '24</u></a>], [<a href="https://arxiv.org/abs/2203.17031"><u>ISCA SPSC '22</u></a>]</li>
	      <li>Robustness Analysis: [<a href="https://ieeexplore.ieee.org/abstract/document/10022646"><u>SLT '22</u></a>]</li>
	      <li>Dataset: [<a href="https://arxiv.org/pdf/2501.08238"><u>arXiv '25</u></a>][<a href="https://arxiv.org/abs/2409.08731">SLT '24</a>]</li>
	   </ul>
	<strong>2) Speech Large Language Models</strong>
	   <ul align="justify">
	      <li>Benchmarking: [<a href="https://arxiv.org/abs/2411.05361">ICLR â€™25</a>], [<a href="https://arxiv.org/abs/2402.13071">ACL Findings â€™24</a>], [<a href="https://arxiv.org/abs/2409.14085">SLT â€™24</a>]</li>
	      <li>Overview / Tech. Report: [<a href="https://arxiv.org/abs/2402.13236">arXiv â€™24</a>], [<a href="https://arxiv.org/abs/2411.07111">arXiv â€™24</a>]</li>
	   </ul>
	<strong>3) Audio Generation </strong>
	   <ul align="justify">
	      <li>Speech Enhancement: [<a href="https://arxiv.org/pdf/2409.10376">ICASSP '25</a>]</li>
	      <li>Source Separation: [<a href="https://ieeexplore.ieee.org/abstract/document/10800383">Oâ€‘COCOSDA '24</a>]</li>
	   </ul>
	</p> -->

	<!-- <h2>Selected Publications</h2> -->
	<!-- <p>* indicates equal contribution</p> -->

        <!-- <h2>Selected Honors</h2> -->
			<!-- <p align="justify">2025: National Science and Technology Council Conference Subsidy </p> -->
			<!-- <p align="justify">2024-25: (2x) NSTC Student Travel Grant, NSTC Taiwan </p> -->

		  
		<!-- <p align="justify">2025: CTCI Foundation Research Scholarship for Overseas Students</p>
	  	<p align="justify">2025: ACLCLP Student Travel Grant, ACLCLP Taiwan</p>
		<p align="justify">2024: Google Student Travel Grant, Google LLC </p>
		<p align="justify">2024: CTCI Foundation Bursary Award for Overseas Students</p>
		<p align="justify">2024-25: (2x) NSTC International Academic Conferences Subsidy </p>
		<p align="justify">2020-25: (5x) Kwong Tung Community Outstanding Student Scholarship </p>
		<p align="justify">2021: Ranked 3rd/42 teams in LA track of ASVspoof 2021 challenge </p>
		<p align="justify">2018-20: Certificate of Achievement, Taiwan Tech (Top 5%, 3 semesters)</p>
	  	<p align="justify">2017: SZIIT Academic Award (3rd Prize), SZIIT</p>
	    <p align="justify">2016: National Encouragement Scholarship, Ministry of Education</p> -->


		  
		<!-- <p align="justify">2017: National Bronze Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016-17: National Encouragement Scholarship; 3rd Prize, SZIIT Acad. Award </p> -->
	      	<!-- <p>2024: National Science and Technology Council (NSTC) Conference Subsidies, NSTC Taiwan </p> -->
		<!-- <p align="justify">2017: National Bronze and Guangdong Provincial Gold Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016: National Encouragement Scholarship</p> -->
		<!-- <p align="justify">2017: 3rd Prize, SZIIT Academic Award</p>
			<p align="justify">2016: National Encouragement Scholarship</p> -->
	      <!-- <p align="justify">2016-2017: National Encouragement Scholarship & 3rd Prize, SZIIT Academic Award </p> -->

		  <br>
      	<!-- <h2>Selected Serivces</h2>
		<!-- <p align="justify">2023-Present: <b>Reviewer/Program Committee</b>, ACL ('24), EMNLP ('24), ICASSP ('23-'25), INTERSPEECH ('25), COLING ('24-'25), MLSP ('24), IALP ('24), ECCV AVGenL ('24)</p> -->
		<!-- <p align="justify">2023-Now: <b>Admin Assistant,</b> <a href="https://nvcenter.ntu.edu.tw/"> NVIDIA-NTU AI Joint Innovation Center</a>, NTU</p> -->
	      	<!-- <p align="justify">2025: <b>Co-Organizer,</b> <a href="https://codecfake.github.io/RespSA-GenAI/"> Responsible Speech & Audio Generative AI,</a> Special Session at IEEE ASRU 2025</p> -->
			<!-- <p align="justify">2024: <b>Technical Committee</b>, <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024: <b>Invited Speaker,</b> <a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Topic: "Singing Voice Graph Modeling for SingFake Detection,"</a> SVDD Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024-25: <b>Teaching Assistant</b>, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php"> Intro. to Generative AI</a> & <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php">Machine Learning</a></p> -->
      		<!-- <p align="justify">2023-Now: <b>Reviewer:</b> AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI, IALP</p>  -->
		  <!-- , ECCV AVGenL -->
		  <!-- <br> -->

		  <h2>Selected Honors</h2>
			<ul>
			  <li>
			    2026: <b>NTU Mr. Wen Tzu-Hsiang Memorial Scholarship</b>, only 9 recipients in NTU.
			  </li>
				<li>
			    2025: <b>Best Student Paper Nominee</b>, The IEEE Autom. Speech Recognit. and Underst. Workshop.
			  </li>
			  <li>
			    2025: <b>Best Paper Award</b>, The 37th Conf. on Comput. Linguist. and Speech Processing.
			  </li>
			  <li>
			    2025: <b>CTCI Foundation Research Scholarship for Overseas Students</b>, awarded by CTCI Foundation 
			  </li>
			  <li>
			    2025: <b>ACLCLP Student Travel Grant</b>, granted by ACLCLP. 
			  </li>
			<li>
			    2024: <b>Google Student Travel Grant</b>, granted by Google LLC. 
			  </li>
			  <li>
			    2024: <b>CTCI Foundation Bursary Award for Overseas Students</b>, awarded by CTCI Foundation 
			  </li>
			  <li>
			    2021: Ranked <b>3rd/42 submissions worldwide</b> on logical access track of ASVspoof 2021 challenge
			  </li>
			  <li>
			    2020-2025: <b>Kwong Tung Community Outstanding Student Scholarship</b>, awarded five years.
			  </li>
			  <li>
			    2018-2020: <b>Certificate of Achievement</b>, Taiwan Tech (Top 5% of studnets; three semesters)
			  </li>
			  <li>
			    2016: <b>National Encouragement Scholarship</b>, Ministry of Education
			  </li>
			</ul>
		  
			  <h2>Selected Services</h2>
				<ul>
				  <li>
				    2025: <b>Co-Organizer</b>, 
				    <a href="https://codecfake.github.io/RespSA-GenAI/">Responsible Speech & Audio Generative AI</a>, 
				    Special Session at IEEE ASRU 2025
				  </li>
				
				  <li>
				    2024: <b>Technical Committee</b>, 
				    <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, 
				    Special Session at IEEE SLT 2024
				  </li>
				
				  <li>
				    2023â€“Now: <b>Reviewer</b>: 
				    AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI
				  </li>
				</ul>
			<footer>
				<br>
				<!-- <p style="text-align: center;"><small>Science, the Endless Frontier.</small></p> -->
				<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cee3f2&w=200&t=tt&d=nEmSewDPXqNGs14be_z5YYcQJ2bmdcQr14eiveUCbnA&co=ffffff&cmn=005fa3&ct=2d78ad&cmo=980000'></script>
			</footer>
		</section>
	</div>
	<script>
      fetch('header.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('sidebar').innerHTML = data;
          
          // --- 1. è¨­å®š About me é«˜äº® ---
          const aboutLink = document.getElementById('nav-about');
          const pubLink = document.getElementById('nav-pub');
          if (aboutLink && pubLink) {
            // About é é¢ï¼Œæ‰€ä»¥ About é«˜äº®
            aboutLink.style.textDecoration = "underline";
            aboutLink.style.textUnderlineOffset = "6px";
            aboutLink.style.color = "#000";
            aboutLink.style.fontWeight = "600";
            // Pub ç°æŽ‰
            pubLink.style.textDecoration = "none";
            pubLink.style.color = "#888";
            pubLink.style.fontWeight = "500";
          }

          // --- 2. ç¶å®š Follow æŒ‰éˆ•äº‹ä»¶ ---
          const followBtn = document.getElementById('follow-btn');
          const socialList = document.getElementById('social-list');
          if (followBtn && socialList) {
            followBtn.addEventListener('click', function(e) {
              e.stopPropagation();
              socialList.classList.toggle('show');
            });
          }
        })
        .catch(err => console.error('Error loading header:', err));

      window.addEventListener('click', function(event) {
        const socialList = document.getElementById('social-list');
        if (socialList && socialList.classList.contains('show')) {
          if (!event.target.matches('#follow-btn')) {
            socialList.classList.remove('show');
          }
        }
      });
    </script>
  </body>
</html>
