<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Xuanjun Chen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
	
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@300;400;600;700&family=Noto+Serif+TC:wght@300;400;600;700&display=swap" rel="stylesheet">
    
	<!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
	<!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet"> -->
	  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
  </head>
  <body>
    <div class="wrapper">
      <header>
     	<div style="text-align: center; margin-top: 10px;">
		<a><img src="img/xj.png" width="130" height="130" style="display: block; margin: 0 auto;" /></a>
		<h2 style="margin: 20px 0;">Xuanjun Chen</h2> <!-- Â¢ûÂä†Ê®ôÈ°åÁöÑ‰∏ä‰∏ãÈñìË∑ù -->
		<p>Ph.D. Student, EECS <br> National Taiwan University</p>
		<div style="margin-top: 0px;">
			<img src="img/ntu_logo.png" 
				alt="National Taiwan University" 
				style="width: 150px; opacity: 0.9;">
		</div>

		<div style="margin-top: 10px;"> <!-- Ê∑ªÂä†ÈñìË∑ù -->
			<a href="mailto:D12942018@ntu.edu.tw" style="margin: 0 10px;">
			    <i class="fas fa-envelope" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://www.linkedin.com/in/jun-ntu/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-linkedin" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://x.com/xjchen_ntu" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-x-twitter" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://github.com/xjchenGit" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-github" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://scholar.google.com/citations?user=ZDVOXd4AAAAJ&hl=en" target="_blank" style="margin: 0 10px;">
			    <i class="fas fa-graduation-cap" style="font-size: 20px; color: #000;"></i>
			</a>
		</div>
		<!-- <br> -->
		<!-- <div style="margin-top: 10px;">
			<a href="img/wechat_qr.JPG" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-weixin" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://www.facebook.com/axuanjunchen/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-facebook" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://www.linkedin.com/in/jun-ntu/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-linkedin" style="font-size: 20px; color: #000;"></i>
			</a>
		</div> -->
		<br>
	</div>

      </header>
      <section>
        <h2>Xuanjun Chen (Èô≥ÁÇ´Âùá)</h2>

        <p align="justify"> I am a Ph.D. student at <a href="https://www.ntu.edu.tw/english/" target="_blank">National Taiwan University (NTU)</a>, where I working with Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" target="_blank">Hung-yi Lee</a> and Prof. <a href="http://mirlab.org/jang/cv/cv.asp" target="_blank">Jyh-Shing Roger Jang</a>. 
			I received M.S. degree from <a href="https://www.ntu.edu.tw/english/" target="_blank">NTU</a> in 2023 and B.S. degree from <a href="https://www.ntust.edu.tw/index.php?Lang=en" target="_blank"> Taiwan Tech </a> in 2020. My research interests include, Speech Processing, Audio/Text LLMs, and Audio Deepfakes. I'm honored to receive <b>Google Student Travel Grant</b> in 2024. </p>
		<br>
		<!-- <h2>Research Interests</h2>
	<p align="justify">My <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> is here! My research interests span deep learning, audio-visual learning, and speech processing. We focus on deepfake detection from a defender‚Äôs perspective, developing methods to counter specific threats like speech synthesis [1], singing voice synthesis [9], and adversarial attacks [2, 8]. Additionally, we‚Äôre creating a lightweight audio-visual synchronization model to identify multi-modal out-of-sync [10] and have developed a dataset [4] incorporating recent advancements in speech synthesis to enhance defense against synthetic voice threats.</p> -->
	<!-- <h2>Research Interests</h2>
	<p align="justify">
		Here is my <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> and research experience:
	<br>
	<strong>1) Multimodal Deepfake Detection</strong>
	   <ul align="justify">
	      <li>Modeling: [<a href="https://arxiv.org/abs/2406.03111"><u>Interspeech '24</u></a>], [<a href="https://arxiv.org/abs/2406.04582"><u>Interspeech '24</u></a>]</li> -->
		<!-- <li>Modeling: <u>SingGraph</u> [<a href="https://arxiv.org/abs/2406.03111">Interspeech '24</a>], [<a href="https://arxiv.org/abs/2406.04582">Interspeech '24</a>]</li> -->
		<!-- <li>Efficiency: [<a href="https://ieeexplore.ieee.org/abstract/document/10446372"><u>ICASSP '24</u></a>], [<a href="https://arxiv.org/abs/2203.17031"><u>ISCA SPSC '22</u></a>]</li>
	      <li>Robustness Analysis: [<a href="https://ieeexplore.ieee.org/abstract/document/10022646"><u>SLT '22</u></a>]</li>
	      <li>Dataset: [<a href="https://arxiv.org/pdf/2501.08238"><u>arXiv '25</u></a>][<a href="https://arxiv.org/abs/2409.08731">SLT '24</a>]</li>
	   </ul>
	<strong>2) Speech Large Language Models</strong>
	   <ul align="justify">
	      <li>Benchmarking: [<a href="https://arxiv.org/abs/2411.05361">ICLR ‚Äô25</a>], [<a href="https://arxiv.org/abs/2402.13071">ACL Findings ‚Äô24</a>], [<a href="https://arxiv.org/abs/2409.14085">SLT ‚Äô24</a>]</li>
	      <li>Overview / Tech. Report: [<a href="https://arxiv.org/abs/2402.13236">arXiv ‚Äô24</a>], [<a href="https://arxiv.org/abs/2411.07111">arXiv ‚Äô24</a>]</li>
	   </ul>
	<strong>3) Audio Generation </strong>
	   <ul align="justify">
	      <li>Speech Enhancement: [<a href="https://arxiv.org/pdf/2409.10376">ICASSP '25</a>]</li>
	      <li>Source Separation: [<a href="https://ieeexplore.ieee.org/abstract/document/10800383">O‚ÄëCOCOSDA '24</a>]</li>
	   </ul>
	</p> -->

	<!-- <h2>Selected Publications</h2> -->
	<!-- <p>* indicates equal contribution</p> -->
        <h2>Selected Publications <small style="font-size: 14px; font-weight: normal;">(* equal contribution)</small></h2>
			<p align="justify"> üí° I am a newcomer to the field of Audio/Text Large Language Models. Prior to this, I spent over 5 years working on Audio Deepfake problems. <p>
		  <h3 style="
			    margin-top: 26px;
			    margin-bottom: 15px;
			    font-size: 15px;
			    font-weight: 600;
			    color: #444;
			    border-left: 3px solid #3a7bd5;
			    padding-left: 10px;">
			    üéôÔ∏è Audio Large Language Models
			</h3>
		<div class="pub-list">
			<p align="justify">
			    	[5] Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, <u>Xuanjun Chen</u>, et al., <b>"DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment"</b> in arXiv 2025. 
				<br>
					<a href="https://arxiv.org/abs/2507.02768">arXiv</a> / 
				<a href="https://github.com/kehanlu/DeSTA2.5-Audio">Code</a>
			</p>
			  
			<p align="justify">	   
			    	[4] Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, <u>Xuanjun Chen</u>, et al., <b>"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"</b> in ICLR 2025.
				<br>
				<a href="https://openreview.net/forum?id=s7lzZpAW7T">OpenReview</a> / 
				<a href="https://arxiv.org/abs/2411.05361v1">arXiv</a> / 
				<a href="https://github.com/dynamic-superb/dynamic-superb">Code</a>
			</p>
	
			<p align="justify">
			    	[3] Haibin Wu, <u>Xuanjun Chen</u>, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, and Hung-yi Lee. <b>"Towards audio language modeling-an overview,"</b> Overview Report, Feb. 2024.
				<br>
					<a href="https://arxiv.org/abs/2402.13236">arXiv</a> /
	                <a href="https://github.com/ga642381/speech-trident">Awesome List</a>
		    	</p>
			    
		        <p align="justify">
			    	[2] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, <u>Xuanjun Chen</u>, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, and Hung-yi Lee. <b>"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models,"</b> in Findings of ACL 2024. 
				<br>
					<a href="https://aclanthology.org/2024.findings-acl.616/">ACL Anthology</a> /
					<a href="https://arxiv.org/abs/2402.13071">arXiv</a> /
					<a href="https://codecsuperb.com/">Leaderboard</a> /
					<a href="https://github.com/voidful/Codec-SUPERB">Code</a> /
					<a href="https://huggingface.co/Codec-SUPERB">Huggingface</a>
		    	</p>
	
		        <p align="justify">
			    	[1] Haibin Wu, <u>Xuanjun Chen</u>, Yi-Cheng Lin, Jiawei Du, Kai-Wei Chang, Ke-Han Lu, Alexander Liu, Ho-Lam Chung, Yuan-Kuei Wu, Dongchao Yang, Songxiang Liu, Yi-Chiao Wu, Xu Tan, James Glass, Shinji Watanabe, and Hung-yi Lee. <b>"Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural codec models,"</b> in IEEE SLT 2024.
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10832364">IEEE Xplore</a> /
				<a href="https://arxiv.org/abs/2409.14085">arXiv</a>
			</p>
		</div>

		  <h3 style="
			    margin-top: 26px;
			    margin-bottom: 15px;
			    font-size: 15px;
			    font-weight: 600;
			    color: #444;
			    border-left: 3px solid #3a7bd5;
			    padding-left: 10px;">
			    üîç Retrieval Augmented Generation
		</h3>

		<p align="justify">
		    	[2] Wei-Chieh Chou*, <u>Xuanjun Chen*</u>, Jian-Ren Lin, Claire Lin, Hung-yi Lee, Jyh-Shing Roger Jang, <b>"Efficient Retrieval-Augmented Generation via Grounded Planning." </b> Working in Progress. 
			<br>
				<!-- <a href="">arXiv</a> -->
		</p>
				
		<p align="justify">
		    	[1] Claire Lin*, Bo-Han Feng*, <u>Xuanjun Chen*</u>, Te-Lun Yang, Hung-yi Lee, Jyh-Shing Roger Jang, <b>"A Preliminary Study of RAG for Taiwanese Historical Archives"</b> in ROCLING 2025 (üèÜ Best Paper Award). 
			<br>
				<a href="https://aclanthology.org/2025.rocling-main.6/">ACL Anthology</a> / 	
				<a href="https://arxiv.org/abs/2511.07445">arXiv</a>
		</p>

		  <h3 style="
			    margin-top: 26px;
			    margin-bottom: 15px;
			    font-size: 15px;
			    font-weight: 600;
			    color: #444;
			    border-left: 3px solid #3a7bd5;
			    padding-left: 10px;">
			    üõ°Ô∏è Audio Deepfake Detection, Localization, Attribution, and Reliability
		</h3>

		<p align="justify">	   
		    	[11] <u>Xuanjun Chen*</u>, Jiawei Du*, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee, <b>"CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset,"</b> Preprint, 2025.
			<br>
			<a href="https://arxiv.org/pdf/2501.08238v2">arXiv</a> / 
			<a href="https://responsiblegenai.github.io/CodecFake-Plus-Dataset/">Project Page</a> / 
			<a href="https://huggingface.co/datasets/CodecFake/CodecFake_Plus_Dataset">Hugging Face</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Plus-Dataset">Code</a>
		</p>
		  
		<p align="justify">
		    	[10] <u>Xuanjun Chen*</u>, Shih-Peng Cheng*, Jiawei Du, Lin Zhang, Xiaoxiao Miao, Chung-Che Wang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling,"</b> in arxiv 2025. 
			<br>
			<a href="https://arxiv.org/abs/2508.02000">arXiv</a>
		</p>

		<p align="justify">
		    	[9] <u>Xuanjun Chen</u>, Chia-Yu Hu, I-Ming Lin, Yi-Cheng Lin, I-Hsiang Chiu, You Zhang, Sung-Feng Huang, Yi-Hsuan Yang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang, <b>"How Does Instrumental Music Help SingFake Detection?"</b> in arXiv 2025. 
			<br>
				<a href="https://arxiv.org/abs/2509.14675">arXiv</a>
		</p>
	      
	  	<p align="justify">
		    	[8] <u>Xuanjun Chen*</u>, I-Ming Lin*, Lin Zhang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Towards Generalized Source Tracing for Codec-Based Deepfake Speech,"</b> in IEEE ASRU 2025 (üèÜ Best Student Paper nominee). 
			<br>
		      	<a href="https://arxiv.org/abs/2506.07294">arXiv</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
		</p>
	      
		<p align="justify">
		    	[7] <u>Xuanjun Chen*</u>, I-Ming Lin*, Lin Zhang, Jiawei Du, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy,"</b> in INTERSPEECH 2025. 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2025/chen25j_interspeech.pdf">ISCA Archive</a> / 
			<a href="https://arxiv.org/pdf/2505.12994">arXiv</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
		</p>
	      
		<!-- <p align="justify">	   
		    	[10] Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, <b>Xuanjun Chen</b>, et al., <b>"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"</b> in ICLR 2025.
			<br>
			<a href="https://arxiv.org/abs/2411.05361v1">arXiv</a> / 
			<a href="https://github.com/dynamic-superb/dynamic-superb">Code</a>
	    	</p> -->

		<!-- <p align="justify">	   
		    	[10] Wenze Ren, Haibin Wu, Yi-Cheng Lin, <b>Xuanjun Chen</b>, Rong Chao, Kuo-Hsuan Hung, You-Jin Li, Wen-Yuan Ting, Hsin-Min Wang, Yu Tsao, <b>"Leveraging Joint Spectral and Spatial Learning with MAMBA for Multichannel Speech Enhancement,"</b> in ICASSP 2025.
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10890412">IEEE Xplore</a> /
			<a href="https://arxiv.org/abs/2409.10376">arXiv</a>
	    	</p> -->

		<!-- <p align="justify">	   
		    	[11] Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, <b>Xuanjun Chen</b>, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee, <b>"Building a Taiwanese Mandarin Spoken Language Model: A First Attempt,"</b> Tech Report, Nov. 2024.
			<br>
			<a href="https://arxiv.org/abs/2411.07111">arXiv</a>
	    	</p> -->

		    
		<p align="justify">
		    	[6] <u>Xuanjun Chen</u>, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Singing Voice Graph Modeling for SingFake Detection,"</b> in INTERSPEECH 2024 (Oral). 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf">ISCA Archive</a> /
			<a href="https://arxiv.org/pdf/2406.03111">arXiv</a> /
			<a href="https://github.com/xjchenGit/SingGraph.git">Code</a> /
			<a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Lightning Talk</a>
	    	</p>
	    	<p align="justify">
		    	[5] <u>Xuanjun Chen*</u>, Jiawei Du*, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Neural Codec-based Adversarial Sample Detection for Speaker Verification,"</b> in INTERSPEECH 2024.
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24p_interspeech.pdf">ISCA Archive</a> /
			<a href="https://arxiv.org/abs/2406.04582">arXiv</a> /
			<a href="https://github.com/hbwu-ntu/spot-adv-by-vocoder.git">Code</a> /
			<a href="">Poster</a>
	    	</p>

		<p align="justify">
		    	[4] Jiawei Du, I-Ming Lin, I-Hsiang Chiu, <u>Xuanjun Chen</u>, Haibin Wu, Wenze Ren, Yu Tsao, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"DFADD: The Diffusion and Flow-Matching based Audio Deepfake Dataset,"</b> in IEEE SLT 2024. 
 			<br>
			<a href="https://ieeexplore.ieee.org/document/10832250">IEEE Xplore</a> /
			<a href="https://www.arxiv.org/abs/2409.08731">arXiv</a> /
			<a href="https://github.com/isjwdu/DFADD">Code</a> /
			<a href="https://huggingface.co/datasets/isjwdu/DFADD">Huggingface</a>
	    	</p>
		<!-- <p align="justify">
		    	[2] Hsuan-Yu Lin, <b>Xuanjun Chen</b>, Jyh-Shing Roger Jang. <b>"Singer separation for karaoke content generation,"</b> in IEEE O-COCOSDA 2024.
			<br>
			<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800383">IEEE Xplore</a> /
			<a href="https://arxiv.org/abs/2110.06707">arXiv</a> /
                        <a href="https://gulaerchen.github.io/">Project</a>
	    	</p> -->
			<p align="justify">	   
		    	[3] <u>Xuanjun Chen</u>, Haibin Wu, Chung-Che Wang, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Multimodal Transformer Distillation for Audio-Visual Synchronization,"</b> in ICASSP 2024.
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10446372">IEEE Xplore</a> /
				<a href="https://arxiv.org/abs/2210.15563">arXiv</a> /
				<a href="https://github.com/xjchenGit/MTDVocaLiST">Code</a> /
				<a href="docs/MTDVocaLiST_poster.pdf">Poster</a>
	    	</p>	
		  
		  <p align="justify">	   
		    	[2] <u>Xuanjun Chen*</u>, Haibin Wu*, Helen Meng, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual Active Speaker Detection,"</b> in IEEE SLT 2022, Jan 2023. 
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10022646">IEEE Xplore</a> /
			<a href="https://arxiv.org/abs/2210.00753">arXiv</a> /
			<a href="https://xjchengit.github.io/Push-Pull/index.html">Demos</a> /
			<a href="docs/2022SLT_Poster_Push_Pull.pdf">Poster</a> /
			<a href="https://www.youtube.com/watch?v=-Yy4TbcPovU">Video</a>
	    	</p>
		    
		<p align="justify">	   
		    	[1] <u>Xuanjun Chen*</u>, Yen-Lun Liao*, Chung-Che Wang, and Jyh-Shing Roger Jang, <b>"Adversarial Speaker Distillation for Countermeasure Model on Automatic Speaker Verification,"</b> in ISCA SPSC 2022. 
			<br>
			<a href="https://www.isca-speech.org/archive/spsc_2022/liao22_spsc.html">ISCA Archive</a> /
			<a href="https://arxiv.org/abs/2203.17031"">arXiv</a>
		</p>


        <!-- <h2>Selected Honors</h2> -->
			<!-- <p align="justify">2025: National Science and Technology Council Conference Subsidy </p> -->
			<!-- <p align="justify">2024-25: (2x) NSTC Student Travel Grant, NSTC Taiwan </p> -->

		  
		<!-- <p align="justify">2025: CTCI Foundation Research Scholarship for Overseas Students</p>
	  	<p align="justify">2025: ACLCLP Student Travel Grant, ACLCLP Taiwan</p>
		<p align="justify">2024: Google Student Travel Grant, Google LLC </p>
		<p align="justify">2024: CTCI Foundation Bursary Award for Overseas Students</p>
		<p align="justify">2024-25: (2x) NSTC International Academic Conferences Subsidy </p>
		<p align="justify">2020-25: (5x) Kwong Tung Community Outstanding Student Scholarship </p>
		<p align="justify">2021: Ranked 3rd/42 teams in LA track of ASVspoof 2021 challenge </p>
		<p align="justify">2018-20: Certificate of Achievement, Taiwan Tech (Top 5%, 3 semesters)</p>
	  	<p align="justify">2017: SZIIT Academic Award (3rd Prize), SZIIT</p>
	    <p align="justify">2016: National Encouragement Scholarship, Ministry of Education</p> -->


		  
		<!-- <p align="justify">2017: National Bronze Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016-17: National Encouragement Scholarship; 3rd Prize, SZIIT Acad. Award </p> -->
	      	<!-- <p>2024: National Science and Technology Council (NSTC) Conference Subsidies, NSTC Taiwan </p> -->
		<!-- <p align="justify">2017: National Bronze and Guangdong Provincial Gold Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016: National Encouragement Scholarship</p> -->
		<!-- <p align="justify">2017: 3rd Prize, SZIIT Academic Award</p>
			<p align="justify">2016: National Encouragement Scholarship</p> -->
	      <!-- <p align="justify">2016-2017: National Encouragement Scholarship & 3rd Prize, SZIIT Academic Award </p> -->

		  <br>
      	<!-- <h2>Selected Serivces</h2>
		<!-- <p align="justify">2023-Present: <b>Reviewer/Program Committee</b>, ACL ('24), EMNLP ('24), ICASSP ('23-'25), INTERSPEECH ('25), COLING ('24-'25), MLSP ('24), IALP ('24), ECCV AVGenL ('24)</p> -->
		<!-- <p align="justify">2023-Now: <b>Admin Assistant,</b> <a href="https://nvcenter.ntu.edu.tw/"> NVIDIA-NTU AI Joint Innovation Center</a>, NTU</p> -->
	      	<!-- <p align="justify">2025: <b>Co-Organizer,</b> <a href="https://codecfake.github.io/RespSA-GenAI/"> Responsible Speech & Audio Generative AI,</a> Special Session at IEEE ASRU 2025</p> -->
			<!-- <p align="justify">2024: <b>Technical Committee</b>, <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024: <b>Invited Speaker,</b> <a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Topic: "Singing Voice Graph Modeling for SingFake Detection,"</a> SVDD Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024-25: <b>Teaching Assistant</b>, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php"> Intro. to Generative AI</a> & <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php">Machine Learning</a></p> -->
      		<!-- <p align="justify">2023-Now: <b>Reviewer:</b> AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI, IALP</p>  -->
		  <!-- , ECCV AVGenL -->
		  <!-- <br> -->

		  <h2>Selected Services</h2>
			<ul>
			  <li>
			    2025: <b>Co-Organizer</b>, 
			    <a href="https://codecfake.github.io/RespSA-GenAI/">Responsible Speech & Audio Generative AI</a>, 
			    Special Session at IEEE ASRU 2025
			  </li>
			
			  <li>
			    2024: <b>Technical Committee</b>, 
			    <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, 
			    Special Session at IEEE SLT 2024
			  </li>
			
			  <li>
			    2023‚ÄìNow: <b>Reviewer</b>: 
			    AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI
			  </li>
			</ul>


	  	<!-- <h2>Selected Honors</h2>
			<p align="justify">2025: Best Student Paper nominee (Top 5%), IEEE ASRU 2025</p>
		  	<p align="justify">2025: Best Paper Award, ROCLING 2025</p>
		  	<p align="justify">2025-2026: NTU Mr. Wen Tzu-Hsiang Memorial Scholarship</p>	
			<p align="justify">2024-2025: CTCI Foundation Research Scholarship & Bursary Award</p>
		  	<p align="justify">2024-2025: Google Student Travel Grant & ACLCLP Student Travel Grant</p>
		  	<p align="justify">2018-20: Certificate of Achievement, Taiwan Tech (Top 5%, 3 semesters)</p>
		  	<p align="justify">2016: National Encouragement Scholarship, Ministry of Education</p> -->

	</section>
      	<footer>
		<br>
		<!-- <p style="text-align: center;"><small>Science, the Endless Frontier.</small></p> -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cee3f2&w=200&t=tt&d=nEmSewDPXqNGs14be_z5YYcQJ2bmdcQr14eiveUCbnA&co=ffffff&cmn=005fa3&ct=2d78ad&cmo=980000'></script>
	</footer>
	</div>
    	<script src="javascripts/scale.fix.js"></script>
  </body>
</html>
