<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>About - Xuanjun Chen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
 	
  	<style>
      .wrapper { display: flex; max-width: 1400px; margin: 0 auto; position: relative; }
      
      header { 
        width: 235px; 
        height: 100vh; 
        position: sticky; 
        top: 0; 
        padding: 40px 0;
        border-image: linear-gradient(to bottom, rgba(238,238,238,0) 0%, rgba(238,238,238,1) 15%, rgba(238,238,238,1) 85%, rgba(238,238,238,0) 100%) 1 100%;
      }
    
      section { flex: 1; padding: 40px 35px; }

		.pub-award {
		  font-weight: 600;
		  color: #000;
		  margin-left: 5px;
		}
    
      /* --- æ–°å¢æ‰‹æ©Ÿç‰ˆ RWD æ§åˆ¶ --- */
      @media (max-width: 768px) {
		body {
			padding: 0 !important; /* ç§»é™¤ body çš„é è¨­å¤§å…§è· */
  		}
		  
        .wrapper {
			display: flex;
            flex-direction: column;
            width: 100% !important;
			max-width: 100% !important; /* ç¢ºä¿ä¸è¢« 1400px é™åˆ¶ */
			padding: 0 !important;
			margin: 0 !important;
        }
        
        header {
            width: 100% !important;
            height: auto !important; /* ç¢ºä¿ä¸ä½”ç”¨ 100vh */
            min-height: 0 !important; /* ç§»é™¤ä»»ä½•æœ€å°é«˜åº¦é™åˆ¶ */
            position: sticky !important;
            top: 0;
            z-index: 1000;
            background: #fff;
            padding: 5px 0 !important; /* ç¸®å°å…§è· */
            border-right: none !important;
            border-bottom: 1px solid #eee !important;
            margin-bottom: 1px solid #eee !important; /* ç¢ºä¿ä¸‹æ–¹ä¸ç•™ç©º */
        }
        
        #nav-menu {
            margin: 0 -20px 20px -20px !important; /* ä½¿ç”¨è² é‚Šè·è®“ç·šæ¢å‘å·¦å³å»¶ä¼¸ */
            padding: 0 20px 12px 20px !important; /* è£œå›å…§è·ç¢ºä¿æ–‡å­—ä¸æœƒè²¼ç‰† */
            /* border-bottom: 1px solid #f0f0f0; */
        }
        
        section {
            width: 100% !important;
            padding-top: 20px !important; /* èª¿æ•´å…§å®¹èˆ‡é ‚éƒ¨å°èˆªçš„é–“è· */
            padding: 20px 15px !important; /* æ‰‹æ©Ÿç‰ˆé–“è·ç¸®å° */
            box-sizing: border-box !important;
			border-top: none !important;
			border-bottom: none !important;
            display: block; 
            flex-direction: column;
        }
		  
        /* é‡å°è«–æ–‡åˆ—è¡¨åœ¨æ‰‹æ©Ÿä¸Šçš„ç¸®æ’å„ªåŒ– */
		.pub-num {
			min-width: 1.5em !important; /* æ‰‹æ©Ÿä¸Šåºè™Ÿå¯¬åº¦ç¨å¾®ç¸®å° */
			margin-right: 0.5em !important;
		}
      }
    </style>
	  
  </head>
  <body>
    <div class="wrapper">
      <header id="sidebar"></header>
		
      <section>
	  	<!-- <nav style="margin-bottom: 30px; font-family: 'Source Serif 4', serif; text-align: right;">
	        <a href="index.html" style="text-decoration: none; color: #3a7bd5; font-weight: 600; margin-right: 15px;">About</a>
	        <a href="publications.html" style="text-decoration: none; color: #666; font-weight: 600;">Publications</a>
	    </nav> -->
        <h2>About</h2>

        <p align="justify"> I am a Ph.D. student at <a href="https://www.ntu.edu.tw/english/" target="_blank">National Taiwan University (NTU)</a>, 
			advised by Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" target="_blank">Hung-yi Lee</a> and 
			Prof. <a href="http://mirlab.org/jang/cv/cv.asp" target="_blank">Jyh-Shing Roger Jang</a>. 
			I received M.S. degree from <a href="https://www.ntu.edu.tw/english/" target="_blank">NTU</a> in 2023 and 
			B.S. degree from <a href="https://www.ntust.edu.tw/index.php?Lang=en" target="_blank"> Taiwan Tech </a> in 2020. 
			My research interests include, Speech Processing, Audio/Text Large Language Models (LLMs), and Audio Deepfakes. 
		</p>

		  <p style="text-align: justify; line-height: 1.6;">
			    I have focused on audio deepfake detection from both singing and speech perspectives. I proposed  
			  	<a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.html" target="_blank">SingGraph</a> 
			  	to bridge music and speech understanding for SingFake detection, and developed <a href="https://arxiv.org/abs/2501.08238" target="_blank">CodecFake+</a>, 
			  	a large-scale dataset using neural codec resynthesis as a scalable proxy for TTS/VC data. 
			    Furthermore, we introduced the source tracing task and  
				<a href="https://arxiv.org/abs/2506.07294" target="_blank">SASTNet</a>, recognized as a 
				<strong>ğŸ† Best Student Paper Nominee</strong> at IEEE ASRU 2025.
			</p>
		    
		    <p style="text-align: justify; line-height: 1.6;">
				Recently, I have expanded my research scope to Audio/Text LLMs, including contributions 
				to <a href="https://arxiv.org/abs/2507.02768" target="_blank">DeSTA2.5</a>, 
				<a href="https://aclanthology.org/2024.findings-acl.616/" target="_blank">Codec-SUPERB</a>, 
				and <a href="https://openreview.net/forum?id=s7lzZpAW7T" target="_blank">Dynamic-SUPERB Phase 2</a>. 
				Meanwhile, I mentor research on RAG, with 
				"<a href="https://aclanthology.org/2025.rocling-main.6/" target="_blank">RAG for Historical Archives</a>," 
				was honored with the ğŸ† <strong>Best Paper Award</strong> at ROCLING 2025.

			</p>
		  
		  <h2>Selected Publications <small style="font-size: 14px; font-weight: normal;">(* equal contribution)</small></h2>

			<div id="pub-container"></div>
			
			<!-- YAML parser -->
			<script src="https://cdn.jsdelivr.net/npm/js-yaml@4/dist/js-yaml.min.js"></script>
			<script>
			    function escapeHtml(str) {
			        return String(str)
			            .replaceAll("&", "&amp;")
			            .replaceAll("<", "&lt;")
			            .replaceAll(">", "&gt;")
			            .replaceAll('"', "&quot;")
			            .replaceAll("'", "&#039;");
			    }
			
			    function renderPub(p, index) {
			        const num = index + 1; // é †åºï¼š[1], [2], [3]...
			        
			        // 1. è™•ç†ä½œè€…å§“åï¼šè‡ªå‹•ç‚ºä½ çš„åå­—åŠ ä¸Šä¸‹åŠƒç·š
			        let authors = p.authors_short || p.authors_full || "";
			        authors = authors.replace(/Xuanjun Chen/g, "<u>Xuanjun Chen</u>");
			
			        const title = p.title ? escapeHtml(p.title) : "";
			        const venue = p.venue ? escapeHtml(p.venue) : "";
			        
			        // 2. è™•ç†çé …èˆ‡å‚™è¨» (åƒè€ƒ publications.html çš„è™•ç†æ–¹å¼)
			        // çé …ç·Šè·Ÿåœ¨ venue ä¹‹å¾Œ
			        const award = p.award ? ` <span class="pub-award" style="font-weight: 600; color: #000; margin-left: 5px;">${p.award}</span>` : "";
			        const note = p.note ? ` <span class="pub-note">(${escapeHtml(p.note)})</span>` : "";
			
			        // 3. è™•ç†é€£çµ
			        const linksHtml = (p.links || [])
			            .map(l => `<a href="${l.url}" target="_blank" rel="noopener noreferrer">${escapeHtml(l.name)}</a>`)
			            .join(" / ");
			
			        // 4. è¿”å› HTML çµæ§‹
					// return `
					//     <li class="pub-item" style="display: flex; margin-bottom: 15px;">
					//         <div class="pub-num" style="min-width: 35px; font-weight: 600;">[${num}]</div>
					//         <div class="pub-text" style="text-align: justify;">
					//             <b class="pub-title">${title}</b>${note}<br>
					//             ${authors ? `<span class="pub-authors" style="display: block; color: #555;">${authors}</span>` : ""}
					//             ${venue ? `<span class="pub-venue"><i>${venue}</i></span>${award}` : ""}
					//             ${linksHtml ? `<span class="pub-links" style="font-size: 0.9em;">${linksHtml}</span>` : ""}
					//         </div>
					//     </li>`;
					return `
					        <li class="pub-item" style="display: flex; margin-bottom: 15px; list-style: none;">
					            <div class="pub-num" style="min-width: 35px; font-weight: 600;">[${num}]</div>
					            <div class="pub-text" style="text-align: justify;">
					                <b class="pub-title">${title}</b>${note}<br>
					                ${authors ? `<span class="pub-authors" style="display: block; color: #555; margin-bottom: 2px;">${authors}</span>` : ""}
					                <span class="pub-venue-row">
					                    ${venue ? `<i>${venue}</i>` : ""}${award}
					                </span>
					                <br>
					                ${linksHtml ? `<span class="pub-links" style="font-size: 0.9em;">${linksHtml}</span>` : ""}
					            </div>
					        </li>`;
			    }
			
			    fetch("selected-pubs.yml")
			        .then(res => res.text())
			        .then(text => {
			            const data = jsyaml.load(text);
			            const pubs = Array.isArray(data) ? data : [];
			
			            document.getElementById("pub-container").innerHTML = `
			                <div class="pub-list">
			                    <ul class="pub-ul" style="list-style: none; padding-left: 0;">
			                        ${pubs.map((p, i) => renderPub(p, i)).join("")}
			                    </ul>
			                </div>
			            `;
			        })
			        .catch(err => {
			            document.getElementById("pub-container").innerHTML =
			                `<p style="color:#b00;">Failed to load publications: ${escapeHtml(err)}</p>`;
			        });
			</script>


		  
		<!-- <div class="pub-list" style="padding-left: 20px;">
			<p align="justify">
			    	[11] Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, <u>Xuanjun Chen</u>, et al., <b>"DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment"</b> in arXiv 2025. 
				<br>
					<a href="https://arxiv.org/abs/2507.02768">arXiv</a> / 
				<a href="https://github.com/kehanlu/DeSTA2.5-Audio">Code</a>
			</p>
			  
			<p align="justify">	   
			    	[10] Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, <u>Xuanjun Chen</u>, et al., <b>"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"</b> in ICLR 2025.
				<br>
				<a href="https://openreview.net/forum?id=s7lzZpAW7T">OpenReview</a> / 
				<a href="https://arxiv.org/abs/2411.05361v1">arXiv</a> / 
				<a href="https://github.com/dynamic-superb/dynamic-superb">Code</a>
			</p>

			<p align="justify">
			    	[9] Claire Lin*, Bo-Han Feng*, <u>Xuanjun Chen*</u>, Te-Lun Yang, Hung-yi Lee, Jyh-Shing Roger Jang, <b>"A Preliminary Study of RAG for Taiwanese Historical Archives"</b> in ROCLING 2025 (ğŸ† Best Paper Award). 
				<br>
					<a href="https://aclanthology.org/2025.rocling-main.6/">ACL Anthology</a> / 	
					<a href="https://arxiv.org/abs/2511.07445">arXiv</a>
			</p>
		</div>

		<p align="justify">	   
		    	[8] <u>Xuanjun Chen*</u>, Jiawei Du*, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee, <b>"CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset,"</b> Preprint, 2025.
			<br>
			<a href="https://arxiv.org/pdf/2501.08238v2">arXiv</a> / 
			<a href="https://responsiblegenai.github.io/CodecFake-Plus-Dataset/">Project Page</a> / 
			<a href="https://huggingface.co/datasets/CodecFake/CodecFake_Plus_Dataset">Hugging Face</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Plus-Dataset">Code</a>
		</p> -->
		  
		<!-- <p align="justify">
		    	[8] <u>Xuanjun Chen*</u>, Shih-Peng Cheng*, Jiawei Du, Lin Zhang, Xiaoxiao Miao, Chung-Che Wang, Haibin Wu, et al., <b>"Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling,"</b> in arxiv 2025. 
			<br>
			<a href="https://arxiv.org/abs/2508.02000">arXiv</a>
		</p> -->

		<!-- <p align="justify">
		    	[7] <u>Xuanjun Chen</u>, Chia-Yu Hu, I-Ming Lin, Yi-Cheng Lin, I-Hsiang Chiu, You Zhang, Sung-Feng Huang, Yi-Hsuan Yang, Haibin Wu, et al., <b>"How Does Instrumental Music Help SingFake Detection?"</b> in ICASSP 2026. 
			<br>
				<a href="https://arxiv.org/abs/2509.14675">arXiv</a>
		</p>
	      
	  	<p align="justify">
		    	[6] <u>Xuanjun Chen*</u>, I-Ming Lin*, Lin Zhang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Towards Generalized Source Tracing for Codec-Based Deepfake Speech,"</b> in IEEE ASRU 2025 (ğŸ† Best Student Paper nominee). 
			<br>
		      	<a href="https://arxiv.org/abs/2506.07294">arXiv</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
		</p>
	      
		<p align="justify">
		    	[5] <u>Xuanjun Chen*</u>, I-Ming Lin*, Lin Zhang, Jiawei Du, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy,"</b> in INTERSPEECH 2025. 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2025/chen25j_interspeech.pdf">ISCA Archive</a> / 
			<a href="https://arxiv.org/pdf/2505.12994">arXiv</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
		</p>

		    
		<p align="justify">
		    	[4] <u>Xuanjun Chen</u>, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Singing Voice Graph Modeling for SingFake Detection,"</b> in INTERSPEECH 2024 (Oral). 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf">ISCA Archive</a> /
			<a href="https://arxiv.org/pdf/2406.03111">arXiv</a> /
			<a href="https://github.com/xjchenGit/SingGraph.git">Code</a> /
			<a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Lightning Talk</a>
	    	</p> -->

			<!-- <p align="justify">	   
		    	[4] <u>Xuanjun Chen</u>, Haibin Wu, Chung-Che Wang, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Multimodal Transformer Distillation for Audio-Visual Synchronization,"</b> in ICASSP 2024.
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10446372">IEEE Xplore</a> /
				<a href="https://arxiv.org/abs/2210.15563">arXiv</a> /
				<a href="https://github.com/xjchenGit/MTDVocaLiST">Code</a> /
				<a href="docs/MTDVocaLiST_poster.pdf">Poster</a>
	    	</p> -->

			<!-- <p align="justify">
				[3] Haibin Wu, <u>Xuanjun Chen</u>, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, and Hung-yi Lee. <b>"Towards audio language modeling-an overview,"</b> Overview Report, Feb. 2024.
				<br>
				<a href="https://arxiv.org/abs/2402.13236">arXiv</a> /
				<a href="https://github.com/ga642381/speech-trident">Awesome List</a>
			</p>
			    
			<p align="justify">
				[2] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, <u>Xuanjun Chen</u>, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, and Hung-yi Lee. <b>"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models,"</b> in Findings of ACL 2024. 
				<br>
				<a href="https://aclanthology.org/2024.findings-acl.616/">ACL Anthology</a> /
				<a href="https://arxiv.org/abs/2402.13071">arXiv</a> /
				<a href="https://codecsuperb.com/">Leaderboard</a> /
				<a href="https://github.com/voidful/Codec-SUPERB">Code</a> /
				<a href="https://huggingface.co/Codec-SUPERB">Huggingface</a>
			</p>
	
			<p align="justify">
				[1] Haibin Wu, <u>Xuanjun Chen</u>, Yi-Cheng Lin, Jiawei Du, Kai-Wei Chang, Ke-Han Lu, Alexander Liu, Ho-Lam Chung, Yuan-Kuei Wu, Dongchao Yang, Songxiang Liu, Yi-Chiao Wu, Xu Tan, James Glass, Shinji Watanabe, and Hung-yi Lee. <b>"Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural codec models,"</b> in IEEE SLT 2024.
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10832364">IEEE Xplore</a> /
				<a href="https://arxiv.org/abs/2409.14085">arXiv</a>
			</p> -->

        <!-- <h2>Selected Honors</h2> -->
			<!-- <p align="justify">2025: National Science and Technology Council Conference Subsidy </p> -->
			<!-- <p align="justify">2024-25: (2x) NSTC Student Travel Grant, NSTC Taiwan </p> -->

		  
		<!-- <p align="justify">2025: CTCI Foundation Research Scholarship for Overseas Students</p>
	  	<p align="justify">2025: ACLCLP Student Travel Grant, ACLCLP Taiwan</p>
		<p align="justify">2024: Google Student Travel Grant, Google LLC </p>
		<p align="justify">2024: CTCI Foundation Bursary Award for Overseas Students</p>
		<p align="justify">2024-25: (2x) NSTC International Academic Conferences Subsidy </p>
		<p align="justify">2020-25: (5x) Kwong Tung Community Outstanding Student Scholarship </p>
		<p align="justify">2021: Ranked 3rd/42 teams in LA track of ASVspoof 2021 challenge </p>
		<p align="justify">2018-20: Certificate of Achievement, Taiwan Tech (Top 5%, 3 semesters)</p>
	  	<p align="justify">2017: SZIIT Academic Award (3rd Prize), SZIIT</p>
	    <p align="justify">2016: National Encouragement Scholarship, Ministry of Education</p> -->


		  
		<!-- <p align="justify">2017: National Bronze Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016-17: National Encouragement Scholarship; 3rd Prize, SZIIT Acad. Award </p> -->
	      	<!-- <p>2024: National Science and Technology Council (NSTC) Conference Subsidies, NSTC Taiwan </p> -->
		<!-- <p align="justify">2017: National Bronze and Guangdong Provincial Gold Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016: National Encouragement Scholarship</p> -->
		<!-- <p align="justify">2017: 3rd Prize, SZIIT Academic Award</p>
			<p align="justify">2016: National Encouragement Scholarship</p> -->
	      <!-- <p align="justify">2016-2017: National Encouragement Scholarship & 3rd Prize, SZIIT Academic Award </p> -->

      	<!-- <h2>Selected Serivces</h2>
		<!-- <p align="justify">2023-Present: <b>Reviewer/Program Committee</b>, ACL ('24), EMNLP ('24), ICASSP ('23-'25), INTERSPEECH ('25), COLING ('24-'25), MLSP ('24), IALP ('24), ECCV AVGenL ('24)</p> -->
		<!-- <p align="justify">2023-Now: <b>Admin Assistant,</b> <a href="https://nvcenter.ntu.edu.tw/"> NVIDIA-NTU AI Joint Innovation Center</a>, NTU</p> -->
	      	<!-- <p align="justify">2025: <b>Co-Organizer,</b> <a href="https://codecfake.github.io/RespSA-GenAI/"> Responsible Speech & Audio Generative AI,</a> Special Session at IEEE ASRU 2025</p> -->
			<!-- <p align="justify">2024: <b>Technical Committee</b>, <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024: <b>Invited Speaker,</b> <a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Topic: "Singing Voice Graph Modeling for SingFake Detection,"</a> SVDD Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024-25: <b>Teaching Assistant</b>, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php"> Intro. to Generative AI</a> & <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php">Machine Learning</a></p> -->
      		<!-- <p align="justify">2023-Now: <b>Reviewer:</b> AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI, IALP</p>  -->
		  <!-- , ECCV AVGenL -->
		  <!-- <br> -->

		<h2>Experience</h2>
			<ul>
				<li>
				    Sep 2023 - Present: Ph.D student in Communication Engineering, , National Taiwan University
				</li>
				<li>
			    	Jan 2023: M.S. in Computer Science and Information Engineering, National Taiwan University
			  	</li>
			  	<li>
			    	Jun 2020: B.S. in Computer Science and Information Engineering, National Taiwan University of Science and Technology
			  	</li>
			</ul>
		  
		<h2>Selected Honors</h2>
			<ul>
			  <li>
			    2026: <b>NTU Mr. Wen Tzu-Hsiang Memorial Scholarship</b>, only 9 recipients in NTU.
			  </li>
				<li>
			    2025: <b>Best Student Paper Nominee</b>, The IEEE Autom. Speech Recognit. and Underst. Workshop.
			  </li>
			  <li>
			    2025: <b>Best Paper Award</b>, The 37th Conf. on Comput. Linguist. and Speech Processing.
			  </li>
			  <li>
			    2025: <b>CTCI Foundation Research Scholarship for Overseas Students</b>, awarded by CTCI Foundation 
			  </li>
			  <li>
			    2025: <b>ACLCLP Student Travel Grant</b>, granted by ACLCLP. 
			  </li>
			<li>
			    2024: <b>Google Student Travel Grant</b>, granted by Google LLC. 
			  </li>
			  <li>
			    2024: <b>CTCI Foundation Bursary Award for Overseas Students</b>, awarded by CTCI Foundation 
			  </li>
			  <li>
			    2021: Ranked <b>3rd/42 submissions worldwide</b> on logical access track of ASVspoof 2021 challenge
			  </li>
			  <li>
			    2020-2025: <b>Kwong Tung Community Outstanding Student Scholarship</b>, awarded five years.
			  </li>
			  <li>
			    2018-2020: <b>Certificate of Achievement</b>, Taiwan Tech (Top 5% of studnets; three semesters)
			  </li>
			  <li>
			    2016: <b>National Encouragement Scholarship</b>, Ministry of Education
			  </li>
			</ul>
		  
			  <h2>Selected Services</h2>
				<ul>
				  <li>
				    2025: <b>Co-Organizer</b>, 
				    <a href="https://codecfake.github.io/RespSA-GenAI/">Responsible Speech & Audio Generative AI</a>, 
				    Special Session at IEEE ASRU 2025
				  </li>
				
				  <li>
				    2024: <b>Technical Committee</b>, 
				    <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, 
				    Special Session at IEEE SLT 2024
				  </li>
				
				  <li>
				    2023â€“Now: <b>Reviewer</b>: 
				    AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI
				  </li>
				</ul>
		</section>
		<!-- <footer>
			<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cee3f2&w=200&t=tt&d=nEmSewDPXqNGs14be_z5YYcQJ2bmdcQr14eiveUCbnA&co=ffffff&cmn=005fa3&ct=2d78ad&cmo=980000'></script>
		</footer> -->
	</div>
	<script>
      fetch('header.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('sidebar').innerHTML = data;
          
          // --- 1. è¨­å®š About me é«˜äº® ---
          const aboutLink = document.getElementById('nav-about');
          const pubLink = document.getElementById('nav-pub');
          if (aboutLink && pubLink) {
            // About é é¢ï¼Œæ‰€ä»¥ About é«˜äº®
            aboutLink.style.textDecoration = "underline";
            aboutLink.style.textUnderlineOffset = "6px";
            aboutLink.style.color = "#000";
            aboutLink.style.fontWeight = "600";
            // Pub ç°æ‰
            pubLink.style.textDecoration = "none";
            pubLink.style.color = "#888";
            pubLink.style.fontWeight = "500";
          }

          // --- 2. ç¶å®š Follow æŒ‰éˆ•äº‹ä»¶ ---
          const followBtn = document.getElementById('follow-btn');
          const socialList = document.getElementById('social-list');
          if (followBtn && socialList) {
            followBtn.addEventListener('click', function(e) {
              e.stopPropagation();
              socialList.classList.toggle('show');
            });
          }
        })
        .catch(err => console.error('Error loading header:', err));

      window.addEventListener('click', function(event) {
        const socialList = document.getElementById('social-list');
        if (socialList && socialList.classList.contains('show')) {
          if (!event.target.matches('#follow-btn')) {
            socialList.classList.remove('show');
          }
        }
      });
    </script>
  </body>
</html>
