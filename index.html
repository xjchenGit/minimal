<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Xuanjun Chen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  	<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  </head>
  <body>
    <div class="wrapper">
      <header>
     	<div style="text-align: center; margin-top: 10px;">
		<a><img src="img/xj.png" width="130" height="130" style="display: block; margin: 0 auto;" /></a>
		<h2 style="margin: 20px 0;">Xuanjun Chen</h2> <!-- 增加標題的上下間距 -->
		<p>Ph.D. Student @ SPML/MIR Lab</p>
		<p>College of EECS, National Taiwan University</p>
		<div style="margin-top: 10px;"> <!-- 添加間距 -->
			<a href="mailto:D12942018@ntu.edu.tw" style="margin: 0 10px;">
			    <i class="fas fa-envelope" style="font-size: 20px; color: #000;"></i>
			</a>
			<!-- <a href="https://x.com/xjchen_ntu" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-x-twitter" style="font-size: 20px; color: #000;"></i>
			</a> -->
			<a href="https://www.linkedin.com/in/jun-ntu/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-linkedin" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://github.com/xjchenGit" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-github" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://scholar.google.com/citations?user=ZDVOXd4AAAAJ&hl=en" target="_blank" style="margin: 0 10px;">
			    <i class="fas fa-graduation-cap" style="font-size: 20px; color: #000;"></i>
			</a>
		</div>
		<br>
	</div>

      </header>
      <section>
        <h2>About me</h2>

        <p align="justify"> I am a Ph.D. student in Electrical Engineering and Computer Science (EECS) at <a href="https://www.ntu.edu.tw/english/" target="_blank">National Taiwan University (NTU)</a>. I am a member of the Speech Processing and Machine Learning (SPML) Lab and  Multimedia Information Retrieval Lab (MIR Lab), where I worked with Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" target="_blank">Hung-yi Lee</a> and Prof. <a href="http://mirlab.org/jang/cv/cv.asp" target="_blank">Jyh-Shing Roger Jang</a>. 
	I received M.S. degree from <a href="https://www.ntu.edu.tw/english/" target="_blank">NTU</a> in 2023 and B.S. degree from <a href="https://www.ntust.edu.tw" target="_blank"> National Taiwan University of Science and Technology (NTUST) </a> in 2020. I'm honored to receive <b>Google Student Travel Grant</b> in 2024. </p>
	<!-- <p align="justify">My <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> is here!</p> -->
	<p align="justify">My <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> is here! My research interests span deep learning, audio-visual learning, and speech processing. We focus on deepfake detection from a defender’s perspective, developing methods to counter specific threats like speech synthesis [1], singing voice synthesis [9], and adversarial attacks [2, 8]. Additionally, we’re creating a lightweight audio-visual synchronization model to identify multi-modal out-of-sync [10] and have developed a dataset [4] incorporating recent advancements in speech synthesis to enhance defense against synthetic voice threats.</p>

	<!-- <h2>Selected Publications</h2> -->
	<!-- <p>* indicates equal contribution</p> -->
        <h2>Publications <small style="font-size: 14px; font-weight: normal;">(* indicates equal contribution)</small></h2>

		<p align="justify">	   
		    	[13] Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, <b>Xuanjun Chen</b>, et al., <b>"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"</b> in ICLR 2025.
			<br>
			<a href="https://arxiv.org/abs/2411.05361v1">arXiv</a> / 
			<a href="https://github.com/dynamic-superb/dynamic-superb">Code</a>
	    	</p>

		<p align="justify">	   
		    	[12] Wenze Ren, Haibin Wu, Yi-Cheng Lin, <b>Xuanjun Chen</b>, Rong Chao, Kuo-Hsuan Hung, You-Jin Li, Wen-Yuan Ting, Hsin-Min Wang, Yu Tsao, <b>"Leveraging Joint Spectral and Spatial Learning with MAMBA for Multichannel Speech Enhancement,"</b> in ICASSP 2025.
			<br>
			<a href="https://arxiv.org/abs/2409.10376">arXiv</a>
	    	</p>

		<p align="justify">	   
		    	[11] Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, <b>Xuanjun Chen</b>, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee, <b>"Building a Taiwanese Mandarin Spoken Language Model: A First Attempt,"</b> Tech Report, Nov. 2024.
			<br>
			<a href="https://arxiv.org/abs/2411.07111">arXiv</a>
	    	</p>
		    
		<p align="justify">	   
		    	[10] <b>Xuanjun Chen</b>, Haibin Wu, Chung-Che Wang, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Multimodal Transformer Distillation for Audio-Visual Synchronization,"</b> in ICASSP 2024.
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10446372">IEEE</a> /
			<a href="https://arxiv.org/abs/2210.15563">arXiv</a> /
			<a href="https://github.com/xjchenGit/MTDVocaLiST">Code</a> /
			<a href="docs/MTDVocaLiST_poster.pdf">Poster</a>
			<!-- <a class="label label-success" href="https://ieeexplore.ieee.org/abstract/document/10446372" target="_blank">IEEE</a>
			<a class="label label-success" href="https://arxiv.org/abs/2210.15563" target="_blank">arXiv</a>
			<a class="demos label" href="https://github.com/xjchenGit/MTDVocaLiST" target="_blank">Code</a>
			<a class="demos label" href="doc/MTDVocaLiST_poster.pdf" target="_blank">Poster</a> -->
	    	</p>
		    
		<p align="justify">
		    	[9] <b>Xuanjun Chen</b>, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Singing Voice Graph Modeling for SingFake Detection,"</b> in Interspeech 2024 (Oral). 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf">ISCA</a> /
			<a href="https://arxiv.org/pdf/2406.03111">arXiv</a> /
			<a href="https://github.com/xjchenGit/SingGraph.git">Code</a>
			<!-- <a class="demos label" href="" target="_blank">Oral</a> -->
			<!-- <a class="label label-success" href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf" target="_blank">ISCA</a>
			<a class="label label-success" href="https://arxiv.org/pdf/2406.03111" target="_blank">arXiv</a>
			<a class="demos label" href="https://github.com/xjchenGit/SingGraph.git" target="_blank">Code</a>
			<a class="demos label" href="" target="_blank">Oral</a> -->
	    	</p>
		    
		<p align="justify">
		    	[8] <b>Xuanjun Chen*</b>, Jiawei Du*, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Neural Codec-based Adversarial Sample Detection for Speaker Verification,"</b> in Interspeech 2024.
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24p_interspeech.pdf">ISCA</a> /
			<a href="https://arxiv.org/abs/2406.04582">arXiv</a> /
			<a href="https://github.com/hbwu-ntu/spot-adv-by-vocoder.git">Code</a> /
			<a href="">Poster</a>
			<!-- <a class="label label-success" href="https://www.isca-archive.org/interspeech_2024/chen24p_interspeech.pdf" target="_blank">ISCA</a>
			<a class="label label-success" href="https://arxiv.org/abs/2406.04582" target="_blank">arXiv</a>
			<a class="demos label" href="https://github.com/hbwu-ntu/spot-adv-by-vocoder.git" target="_blank">Code</a>
			<a class="demos label" href="" target="_blank">Poster</a> -->
	    	</p>
		    
		<p align="justify">
		    	[7] Haibin Wu, <b>Xuanjun Chen</b>, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, and Hung-yi Lee. <b>"Towards audio language modeling-an overview,"</b> Overview Report, Feb. 2024.
			<br>
			<a href="https://arxiv.org/abs/2402.13236">arXiv</a> /
                        <a href="https://github.com/ga642381/speech-trident">Awesome List</a>
			<!-- <a class="label label-success" href="https://arxiv.org/abs/2402.13236" target="_blank">arXiv</a>
                        <a class="demos label" href="https://github.com/ga642381/speech-trident" target="_blank">Awesome List</a> -->
	    	</p>
		    
	        <p align="justify">
		    	[6] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, <b>Xuanjun Chen</b>, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, and Hung-yi Lee. <b>"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models,"</b> in Findings of ACL 2024. 
			<br>
			<a href="https://aclanthology.org/2024.findings-acl.616/">ACL</a> /
			<a href="https://arxiv.org/abs/2402.13071">arXiv</a> /
			<a href="https://codecsuperb.com/">Leaderboard</a> /
			<a href="https://github.com/voidful/Codec-SUPERB">Code</a> /
			<a href="https://huggingface.co/Codec-SUPERB">Huggingface</a>
			<!-- <a class="label label-success" href="https://aclanthology.org/2024.findings-acl.616/" target="_blank">ACL</a>
			<a class="label label-success" href="https://arxiv.org/abs/2402.13071" target="_blank">arXiv</a>
			<a class="demos label" href="https://codecsuperb.com/" target="_blank">Leaderboard</a>
			<a class="demos label" href="https://github.com/voidful/Codec-SUPERB" target="_blank">Code</a>
			<a class="demos label" href="https://huggingface.co/Codec-SUPERB" target="_blank">Huggingface</a> -->
	    	</p>

	        <p align="justify">
		    	[5] Haibin Wu, <b>Xuanjun Chen</b>, Yi-Cheng Lin, Jiawei Du, Kai-Wei Chang, Ke-Han Lu, Alexander Liu, Ho-Lam Chung, Yuan-Kuei Wu, Dongchao Yang, Songxiang Liu, Yi-Chiao Wu, Xu Tan, James Glass, Shinji Watanabe, and Hung-yi Lee. <b>"Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural codec models,"</b> in SLT 2024.
			<br>
			<a href="https://arxiv.org/abs/2409.14085">arXiv</a>
			<!-- <a class="label label-success" href="https://arxiv.org/abs/2402.13071" target="_blank">arXiv</a> -->
			<!-- <a class="demos label" href="https://codecsuperb.com/" target="_blank">Leaderboard</a>
			<a class="demos label" href="https://github.com/voidful/Codec-SUPERB" target="_blank">Code</a>
			<a class="demos label" href="https://huggingface.co/Codec-SUPERB" target="_blank">Huggingface</a> -->
	    	</p>

		<p align="justify">
		    	[4] Jiawei Du, I-Ming Lin, I-Hsiang Chiu, <b>Xuanjun Chen</b>, Haibin Wu, Wenze Ren, Yu Tsao, Hung-yi Lee, and Jyh-Shing Roger Jang. <b>"DFADD: The Diffusion and Flow-Matching based Audio Deepfake Dataset,"</b> in SLT 2024. 
 			<br>
			<a href="https://www.arxiv.org/abs/2409.08731">arXiv</a> /
			<a href="https://github.com/isjwdu/DFADD">Code</a> /
			<a href="https://huggingface.co/datasets/isjwdu/DFADD">Huggingface</a>
			<!-- <a href="">Poster</a> -->
			<!-- <a class="label label-success" href="https://www.arxiv.org/abs/2409.08731" target="_blank">arXiv</a>
			<a class="demos label" href="https://github.com/isjwdu/DFADD" target="_blank">Code</a>
			<a class="demos label" href="" target="_blank">Poster</a> -->
	    	</p>
		<p align="justify">
		    	[3] Hsuan-Yu Lin, <b>Xuanjun Chen</b>, Jyh-Shing Roger Jang. <b>"Singer separation for karaoke content generation,"</b> in O-COCOSDA 2024.
			<br>
			<a href="https://arxiv.org/abs/2110.06707">arXiv</a> /
                        <a href="https://gulaerchen.github.io/">Project</a>
			<!-- <a class="label label-success" href="https://arxiv.org/abs/2110.06707" target="_blank">arXiv</a>
                        <a class="demos label" href="https://gulaerchen.github.io/" target="_blank">Project</a> -->
	    	</p>
	    	<p align="justify">	   
		    	[2] <b>Xuanjun Chen*</b>, Haibin Wu*, Helen Meng, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual Active Speaker Detection,"</b> in SLT 2022, Jan 2023. 
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10022646">IEEE</a> /
			<a href="https://arxiv.org/abs/2210.00753">arXiv</a> /
			<a href="https://xjchengit.github.io/Push-Pull/index.html">Demos</a> /
			<a href="docs/2022SLT_Poster_Push_Pull.pdf">Poster</a> /
			<a href="https://www.youtube.com/watch?v=-Yy4TbcPovU">Video</a>
			<!-- <a class="label label-success" href="https://ieeexplore.ieee.org/abstract/document/10022646" target="_blank">IEEE</a>
			<a class="label label-success" href="https://arxiv.org/abs/2210.00753" target="_blank">arXiv</a>
			<a class="demos label" href="https://xjchengit.github.io/Push-Pull/index.html" target="_blank">Demos</a>
			<a class="demos label" href="doc/2022SLT_Poster_Push_Pull.pdf" target="_blank">Poster</a>
			<a class="demos label" href="https://www.youtube.com/watch?v=-Yy4TbcPovU" target="_blank">Video</a> -->

	    	</p>
		    
		<p align="justify">	   
		    	[1] <b>Xuanjun Chen*</b>, Yen-Lun Liao*, Chung-Che Wang, and Jyh-Shing Roger Jang, <b>"Adversarial Speaker Distillation for Countermeasure Model on Automatic Speaker Verification,"</b> in ISCA SPSC 2022, Sept 2022. 
			<br>
			<a href="https://www.isca-speech.org/archive/spsc_2022/liao22_spsc.html">ISCA</a> /
			<a href="https://arxiv.org/abs/2203.17031"">arXiv</a>
			<!-- <a class="label label-success" href="https://www.isca-speech.org/archive/spsc_2022/liao22_spsc.html" target="_blank">ISCA</a>
			<a class="label label-success" href="https://arxiv.org/abs/2203.17031"" target="_blank">arXiv</a> -->
	    	</p>
        <h2>Awards</h2>
	      	<p align="justify">2024: Google Student Travel Grant, Google LLC </p>
			<!-- <p>2024: National Science and Technology Council (NSTC) Conference Subsidies, NSTC Taiwan </p> -->
		<p align="justify">2024: Bursary Award for Overseas Students, CTCI Foundation </p>
		<p align="justify">2020 - 2025: Distinguished Academic Record Award (5 years), Taipei Kwong Tong Community Associations </p>
		<p align="justify">2021: Ranked 3rd/42 teams in the logical access track of ASVspoof 2021 challenge, Interspeech 2021 </p>
		<p align="justify">2018 - 2020: Certificate of Achievement (3 semesters), NTUST (Top 5%)</p>
		<p align="justify">2017: National Bronze and Guangdong Provincial Gold Awards, 3rd China College Internet Entrep. Comp. </p>
		<p align="justify">2016-2017: National Encouragement Scholarship (Only 3%) and The Third Prize of Academic Award (Top 20%), SZIIT</p>
      	<h2>Serivces</h2>
	      	<p align="justify">2023-Present: <b>Reviewer/Program Committee</b>, ACL ('24), EMNLP ('25), ICASSP ('23-'25), LREC-COLING/COLING ('24-'25), MLSP ('24), IALP ('24), ECCV AVGenL ('24)</p>
		<p align="justify">2024: <b>Invited Talker,</b> <a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html"><b>Topic:</b> "Singing Voice Graph Modeling for SingFake Detection,"</a> Special Session: SVDD @ IEEE SLT 2024</p>
		<p align="justify">2024: <b>Technical Committee</b>, <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a> at IEEE SLT 2024</p>
		<p align="justify">2024-2025: <b>Teaching Assistant</b>, EE5200: <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php"> Introduction to Generative AI (2024 Spr.) </a> and EE5184: Machine Learning (2025 Spr.), NTU</p>
      	<br>
	</section>
      	<footer>
		<br>
		<p style="text-align: center;"><small>Science, the Endless Frontier.</small></p>
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cee3f2&w=200&t=tt&d=nEmSewDPXqNGs14be_z5YYcQJ2bmdcQr14eiveUCbnA&co=ffffff&cmn=005fa3&ct=2d78ad&cmo=980000'></script>
	</footer>
	</div>
    	<script src="javascripts/scale.fix.js"></script>
  </body>
</html>
