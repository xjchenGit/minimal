<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Xuanjun Chen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
	<!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet"> -->
	  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
  </head>
  <body>
    <div class="wrapper">
      <header>
     	<div style="text-align: center; margin-top: 10px;">
		<a><img src="img/xj.png" width="130" height="130" style="display: block; margin: 0 auto;" /></a>
		<h2 style="margin: 20px 0;">Xuanjun Chen</h2> <!-- 增加標題的上下間距 -->
		<p>Ph.D. Student, EECS <br> National Taiwan University</p>
		<div style="margin-top: 10px;"> <!-- 添加間距 -->
			<a href="mailto:D12942018@ntu.edu.tw" style="margin: 0 10px;">
			    <i class="fas fa-envelope" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://www.linkedin.com/in/jun-ntu/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-linkedin" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://x.com/xjchen_ntu" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-x-twitter" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://github.com/xjchenGit" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-github" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://scholar.google.com/citations?user=ZDVOXd4AAAAJ&hl=en" target="_blank" style="margin: 0 10px;">
			    <i class="fas fa-graduation-cap" style="font-size: 20px; color: #000;"></i>
			</a>
		</div>
		<!-- <br> -->
		<!-- <div style="margin-top: 10px;">
			<a href="img/wechat_qr.JPG" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-weixin" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://www.facebook.com/axuanjunchen/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-facebook" style="font-size: 20px; color: #000;"></i>
			</a>
			<a href="https://www.linkedin.com/in/jun-ntu/" target="_blank" style="margin: 0 10px;">
			    <i class="fab fa-linkedin" style="font-size: 20px; color: #000;"></i>
			</a>
		</div> -->
		<br>
	</div>

      </header>
      <section>
        <h2>About me</h2>

        <p align="justify"> I am a Ph.D. student in Electrical Engineering and Computer Science (EECS) at <a href="https://www.ntu.edu.tw/english/" target="_blank">National Taiwan University (NTU)</a>, where I working with Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" target="_blank">Hung-yi Lee</a> and Prof. <a href="http://mirlab.org/jang/cv/cv.asp" target="_blank">Jyh-Shing Roger Jang</a>. 
	I received M.S. degree from <a href="https://www.ntu.edu.tw/english/" target="_blank">NTU</a> in 2023 and B.S. degree from <a href="https://www.ntust.edu.tw/index.php?Lang=en" target="_blank"> Taiwan Tech </a> in 2020. My research interests include but are not limited to speech processing and machine learning. I'm honored to receive <b>Google Student Travel Grant</b> in 2024. </p>
	
	<!-- <h2>Research Interests</h2>
	<p align="justify">My <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> is here! My research interests span deep learning, audio-visual learning, and speech processing. We focus on deepfake detection from a defender’s perspective, developing methods to counter specific threats like speech synthesis [1], singing voice synthesis [9], and adversarial attacks [2, 8]. Additionally, we’re creating a lightweight audio-visual synchronization model to identify multi-modal out-of-sync [10] and have developed a dataset [4] incorporating recent advancements in speech synthesis to enhance defense against synthetic voice threats.</p> -->
	<!-- <h2>Research Interests</h2>
	<p align="justify">
		Here is my <a href="https://xjchen.tech/docs/xuanjun's_CV.pdf" target="_blank">Curriculum Vitae</a> and research experience:
	<br>
	<strong>1) Multimodal Deepfake Detection</strong>
	   <ul align="justify">
	      <li>Modeling: [<a href="https://arxiv.org/abs/2406.03111"><u>Interspeech '24</u></a>], [<a href="https://arxiv.org/abs/2406.04582"><u>Interspeech '24</u></a>]</li> -->
		<!-- <li>Modeling: <u>SingGraph</u> [<a href="https://arxiv.org/abs/2406.03111">Interspeech '24</a>], [<a href="https://arxiv.org/abs/2406.04582">Interspeech '24</a>]</li> -->
		<!-- <li>Efficiency: [<a href="https://ieeexplore.ieee.org/abstract/document/10446372"><u>ICASSP '24</u></a>], [<a href="https://arxiv.org/abs/2203.17031"><u>ISCA SPSC '22</u></a>]</li>
	      <li>Robustness Analysis: [<a href="https://ieeexplore.ieee.org/abstract/document/10022646"><u>SLT '22</u></a>]</li>
	      <li>Dataset: [<a href="https://arxiv.org/pdf/2501.08238"><u>arXiv '25</u></a>][<a href="https://arxiv.org/abs/2409.08731">SLT '24</a>]</li>
	   </ul>
	<strong>2) Speech Large Language Models</strong>
	   <ul align="justify">
	      <li>Benchmarking: [<a href="https://arxiv.org/abs/2411.05361">ICLR ’25</a>], [<a href="https://arxiv.org/abs/2402.13071">ACL Findings ’24</a>], [<a href="https://arxiv.org/abs/2409.14085">SLT ’24</a>]</li>
	      <li>Overview / Tech. Report: [<a href="https://arxiv.org/abs/2402.13236">arXiv ’24</a>], [<a href="https://arxiv.org/abs/2411.07111">arXiv ’24</a>]</li>
	   </ul>
	<strong>3) Audio Generation </strong>
	   <ul align="justify">
	      <li>Speech Enhancement: [<a href="https://arxiv.org/pdf/2409.10376">ICASSP '25</a>]</li>
	      <li>Source Separation: [<a href="https://ieeexplore.ieee.org/abstract/document/10800383">O‑COCOSDA '24</a>]</li>
	   </ul>
	</p> -->

	<!-- <h2>Selected Publications</h2> -->
	<!-- <p>* indicates equal contribution</p> -->
        <h2>Selected Publications <small style="font-size: 14px; font-weight: normal;">(* equal contribution)</small></h2>

		<p align="justify">
		    	[14] <b>Xuanjun Chen</b>, I-Ming Lin, Lin Zhang, Jiawei Du, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy,"</b> in Interspeech 2025. 
			<br>
			<a href="https://arxiv.org/pdf/2505.12994">arXiv</a> / 
			<!-- <a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf">ISCA</a> / -->
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
	    	</p>
	      
		<p align="justify">	   
		    	[13] <b>Xuanjun Chen</b>, Jiawei Du, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee, <b>"CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset,"</b> Preprint, 2025.
			<br>
			<a href="https://arxiv.org/pdf/2501.08238v2">arXiv</a>
	    	</p>
	      
		<p align="justify">	   
		    	[12] Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, <b>Xuanjun Chen</b>, et al., <b>"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"</b> in ICLR 2025.
			<br>
			<a href="https://arxiv.org/abs/2411.05361v1">arXiv</a> / 
			<a href="https://github.com/dynamic-superb/dynamic-superb">Code</a>
	    	</p>

		<p align="justify">	   
		    	[11] Wenze Ren, Haibin Wu, Yi-Cheng Lin, <b>Xuanjun Chen</b>, Rong Chao, Kuo-Hsuan Hung, You-Jin Li, Wen-Yuan Ting, Hsin-Min Wang, Yu Tsao, <b>"Leveraging Joint Spectral and Spatial Learning with MAMBA for Multichannel Speech Enhancement,"</b> in ICASSP 2025.
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10890412">IEEE</a> /
			<a href="https://arxiv.org/abs/2409.10376">arXiv</a>
	    	</p>

		<!-- <p align="justify">	   
		    	[11] Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, <b>Xuanjun Chen</b>, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee, <b>"Building a Taiwanese Mandarin Spoken Language Model: A First Attempt,"</b> Tech Report, Nov. 2024.
			<br>
			<a href="https://arxiv.org/abs/2411.07111">arXiv</a>
	    	</p> -->
		    
		<p align="justify">	   
		    	[10] <b>Xuanjun Chen</b>, Haibin Wu, Chung-Che Wang, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Multimodal Transformer Distillation for Audio-Visual Synchronization,"</b> in ICASSP 2024.
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10446372">IEEE</a> /
			<a href="https://arxiv.org/abs/2210.15563">arXiv</a> /
			<a href="https://github.com/xjchenGit/MTDVocaLiST">Code</a> /
			<a href="docs/MTDVocaLiST_poster.pdf">Poster</a>
	    	</p>
		    
		<p align="justify">
		    	[9] <b>Xuanjun Chen</b>, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Singing Voice Graph Modeling for SingFake Detection,"</b> in Interspeech 2024 (Oral). 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf">ISCA</a> /
			<a href="https://arxiv.org/pdf/2406.03111">arXiv</a> /
			<a href="https://github.com/xjchenGit/SingGraph.git">Code</a>
	    	</p>
		    
		<p align="justify">
		    	[8] <b>Xuanjun Chen*</b>, Jiawei Du*, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Neural Codec-based Adversarial Sample Detection for Speaker Verification,"</b> in Interspeech 2024.
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24p_interspeech.pdf">ISCA</a> /
			<a href="https://arxiv.org/abs/2406.04582">arXiv</a> /
			<a href="https://github.com/hbwu-ntu/spot-adv-by-vocoder.git">Code</a> /
			<a href="">Poster</a>
	    	</p>
		    
		<p align="justify">
		    	[7] Haibin Wu, <b>Xuanjun Chen</b>, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, and Hung-yi Lee. <b>"Towards audio language modeling-an overview,"</b> Overview Report, Feb. 2024.
			<br>
			<a href="https://arxiv.org/abs/2402.13236">arXiv</a> /
                        <a href="https://github.com/ga642381/speech-trident">Awesome List</a>
	    	</p>
		    
	        <p align="justify">
		    	[6] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, <b>Xuanjun Chen</b>, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, and Hung-yi Lee. <b>"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models,"</b> in Findings of ACL 2024. 
			<br>
			<a href="https://aclanthology.org/2024.findings-acl.616/">ACL</a> /
			<a href="https://arxiv.org/abs/2402.13071">arXiv</a> /
			<a href="https://codecsuperb.com/">Leaderboard</a> /
			<a href="https://github.com/voidful/Codec-SUPERB">Code</a> /
			<a href="https://huggingface.co/Codec-SUPERB">Huggingface</a>
	    	</p>

	        <p align="justify">
		    	[5] Haibin Wu, <b>Xuanjun Chen</b>, Yi-Cheng Lin, Jiawei Du, Kai-Wei Chang, Ke-Han Lu, Alexander Liu, Ho-Lam Chung, Yuan-Kuei Wu, Dongchao Yang, Songxiang Liu, Yi-Chiao Wu, Xu Tan, James Glass, Shinji Watanabe, and Hung-yi Lee. <b>"Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural codec models,"</b> in IEEE SLT 2024.
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10832364">IEEE</a> /
			<a href="https://arxiv.org/abs/2409.14085">arXiv</a>
	    	</p>

		<p align="justify">
		    	[4] Jiawei Du, I-Ming Lin, I-Hsiang Chiu, <b>Xuanjun Chen</b>, Haibin Wu, Wenze Ren, Yu Tsao, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"DFADD: The Diffusion and Flow-Matching based Audio Deepfake Dataset,"</b> in IEEE SLT 2024. 
 			<br>
			<a href="https://ieeexplore.ieee.org/document/10832250">IEEE</a> /
			<a href="https://www.arxiv.org/abs/2409.08731">arXiv</a> /
			<a href="https://github.com/isjwdu/DFADD">Code</a> /
			<a href="https://huggingface.co/datasets/isjwdu/DFADD">Huggingface</a>
	    	</p>
		<p align="justify">
		    	[3] Hsuan-Yu Lin, <b>Xuanjun Chen</b>, Jyh-Shing Roger Jang. <b>"Singer separation for karaoke content generation,"</b> in IEEE O-COCOSDA 2024.
			<br>
			<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800383">IEEE</a> /
			<a href="https://arxiv.org/abs/2110.06707">arXiv</a> /
                        <a href="https://gulaerchen.github.io/">Project</a>
	    	</p>
	    	<p align="justify">	   
		    	[2] <b>Xuanjun Chen*</b>, Haibin Wu*, Helen Meng, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual Active Speaker Detection,"</b> in IEEE SLT 2022, Jan 2023. 
			<br>
			<a href="https://ieeexplore.ieee.org/abstract/document/10022646">IEEE</a> /
			<a href="https://arxiv.org/abs/2210.00753">arXiv</a> /
			<a href="https://xjchengit.github.io/Push-Pull/index.html">Demos</a> /
			<a href="docs/2022SLT_Poster_Push_Pull.pdf">Poster</a> /
			<a href="https://www.youtube.com/watch?v=-Yy4TbcPovU">Video</a>
	    	</p>
		    
		<p align="justify">	   
		    	[1] <b>Xuanjun Chen*</b>, Yen-Lun Liao*, Chung-Che Wang, and Jyh-Shing Roger Jang, <b>"Adversarial Speaker Distillation for Countermeasure Model on Automatic Speaker Verification,"</b> in ISCA SPSC 2022, Sept 2022. 
			<br>
			<a href="https://www.isca-speech.org/archive/spsc_2022/liao22_spsc.html">ISCA</a> /
			<a href="https://arxiv.org/abs/2203.17031"">arXiv</a>
	    	</p>
        <h2>Selected Honors</h2>
	      	<p align="justify">2024: Google Student Travel Grant, Google LLC </p>
		<p align="justify">2024: CTCI Foundation Bursary Award for Overseas Students</p>
		<p align="justify">2020-25: Kwong Tung Community Outstanding Student Scholarship </p>
		<p align="justify">2021: Ranked 3rd/42 teams in LA track of ASVspoof 2021 challenge </p>
		<p align="justify">2018-20: Certificate of Achievement, Taiwan Tech (Top 5%, 3 semesters)</p>
	      	<p align="justify">2016: National Encouragement Scholarship, Chinese Ministry of Education</p>
		<!-- <p align="justify">2017: National Bronze Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016-17: National Encouragement Scholarship; 3rd Prize, SZIIT Acad. Award </p> -->
	      	<!-- <p>2024: National Science and Technology Council (NSTC) Conference Subsidies, NSTC Taiwan </p> -->
		<!-- <p align="justify">2017: National Bronze and Guangdong Provincial Gold Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016: National Encouragement Scholarship</p> -->
		<!-- <p align="justify">2017: 3rd Prize, SZIIT Academic Award</p>
	      	<p align="justify">2016: National Encouragement Scholarship</p> -->
	      <!-- <p align="justify">2016-2017: National Encouragement Scholarship & 3rd Prize, SZIIT Academic Award </p> -->
      	<h2>Selected Serivces</h2>
		<!-- <p align="justify">2023-Present: <b>Reviewer/Program Committee</b>, ACL ('24), EMNLP ('24), ICASSP ('23-'25), Interspeech ('25), COLING ('24-'25), MLSP ('24), IALP ('24), ECCV AVGenL ('24)</p> -->
	      	<p align="justify">2023-Present: <b>Reviewer:</b> IJPRAI, ACL, EMNLP, ICASSP, Interspeech, COLING, MLSP, IALP, ECCV AVGenL</p>
		<p align="justify">2023-Present: <b>Administrative Assistant,</b> <a href="https://nvcenter.ntu.edu.tw/"> NVIDIA‑NTU Artificial Intelligence Joint Research Center,</a> NTU</p>
	      	<p align="justify">2025: <b>Co-Organizer,</b> <a href="https://codecfake.github.io/RespSA-GenAI/"> Responsible Speech and Audio Generative AI,</a> Special Session at IEEE ASRU 2025</p>
		<p align="justify">2024: <b>Technical Committee</b>, <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, Special Session at IEEE SLT 2024</p>
	      	<p align="justify">2024: <b>Invited Speaker,</b> <a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Topic: "Singing Voice Graph Modeling for SingFake Detection,"</a> SVDD Special Session at IEEE SLT 2024</p>
		<p align="justify">2024-25: <b>Teaching Assistant</b>, EE5200: <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php"> Introduction to Generative AI (2024 Spr.) </a> and EE5184: <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php">Machine Learning (2025 Spr.)</a>, NTU</p>
      	<br>
	</section>
      	<footer>
		<br>
		<!-- <p style="text-align: center;"><small>Science, the Endless Frontier.</small></p> -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cee3f2&w=200&t=tt&d=nEmSewDPXqNGs14be_z5YYcQJ2bmdcQr14eiveUCbnA&co=ffffff&cmn=005fa3&ct=2d78ad&cmo=980000'></script>
	</footer>
	</div>
    	<script src="javascripts/scale.fix.js"></script>
  </body>
</html>
