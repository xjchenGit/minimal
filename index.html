<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>About - Xuanjun Chen</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
 	
  	<style>
      .wrapper { display: flex; max-width: 1400px; margin: 0 auto; position: relative; }
      
      header { 
        width: 235px; 
        height: 100vh; 
        position: sticky; 
        top: 0; 
        padding: 40px 0;
        border-image: linear-gradient(to bottom, rgba(238,238,238,0) 0%, rgba(238,238,238,1) 15%, rgba(238,238,238,1) 85%, rgba(238,238,238,0) 100%) 1 100%;
      }
    
      section { flex: 1; padding: 40px 35px; }
    
      /* --- Êñ∞Â¢ûÊâãÊ©üÁâà RWD ÊéßÂà∂ --- */
      @media (max-width: 768px) {
        .wrapper {
              display: flex;
              flex-direction: column;
              width: 100% !important;
              padding: 0 !important;
        }
        
        header {
            width: 100% !important;
            height: auto !important; /* Á¢∫‰øù‰∏ç‰ΩîÁî® 100vh */
            min-height: 0 !important; /* ÁßªÈô§‰ªª‰ΩïÊúÄÂ∞èÈ´òÂ∫¶ÈôêÂà∂ */
            position: sticky !important;
            top: 0;
            z-index: 1000;
            background: #fff;
            padding: 5px 0 !important; /* Á∏ÆÂ∞èÂÖßË∑ù */
            border-right: none !important;
            border-bottom: 1px solid #eee !important;
            margin-bottom: 1px solid #eee !important; /* Á¢∫‰øù‰∏ãÊñπ‰∏çÁïôÁ©∫ */
        }
        
        #nav-menu {
            margin: 0 -20px 20px -20px !important; /* ‰ΩøÁî®Ë≤†ÈÇäË∑ùËÆìÁ∑öÊ¢ùÂêëÂ∑¶Âè≥Âª∂‰º∏ */
            padding: 0 20px 12px 20px !important; /* Ë£úÂõûÂÖßË∑ùÁ¢∫‰øùÊñáÂ≠ó‰∏çÊúÉË≤ºÁâÜ */
            /* border-bottom: 1px solid #f0f0f0; */
        }
        
        section {
            width: 100% !important;
            padding-top: 20px !important; /* Ë™øÊï¥ÂÖßÂÆπËàáÈ†ÇÈÉ®Â∞éËà™ÁöÑÈñìË∑ù */
            padding: 20px !important; /* ÊâãÊ©üÁâàÈñìË∑ùÁ∏ÆÂ∞è */
            box-sizing: border-box !important;
			border-top: none !important;
			border-bottom: none !important;
            display: flex; 
            flex-direction: column;
        }
        /* Âú∞ÂúñÔºöÂú®ÁßªÂãïÁ´ØÂº∑Âà∂ÊéíÂú®ÊúÄ‰∏ãÈù¢ */
      }
    </style>
	  
  </head>
  <body>
    <div class="wrapper">
      <header id="sidebar"></header>
		
      <section>
	  	<!-- <nav style="margin-bottom: 30px; font-family: 'Source Serif 4', serif; text-align: right;">
	        <a href="index.html" style="text-decoration: none; color: #3a7bd5; font-weight: 600; margin-right: 15px;">About</a>
	        <a href="publications.html" style="text-decoration: none; color: #666; font-weight: 600;">Publications</a>
	    </nav> -->
        <h2>About</h2>

        <p align="justify"> I am a Ph.D. student at <a href="https://www.ntu.edu.tw/english/" target="_blank">National Taiwan University (NTU)</a>, advised by Prof. <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" target="_blank">Hung-yi Lee</a> and Prof. <a href="http://mirlab.org/jang/cv/cv.asp" target="_blank">Jyh-Shing Roger Jang</a>. 
			I received M.S. degree from <a href="https://www.ntu.edu.tw/english/" target="_blank">NTU</a> in 2023 and B.S. degree from <a href="https://www.ntust.edu.tw/index.php?Lang=en" target="_blank"> Taiwan Tech </a> in 2020. My research interests include, Speech Processing, Audio/Text Large Language Models (LLMs), and Audio Deepfakes. </p>
			<!-- I'm honored to receive <b>Google Student Travel Grant</b> in 2024. </p> -->
	  	<!-- <p align="justify"> üí° I am a newcomer to the field of Audio/Text Large Language Models. Prior to this, I spent over 5 years working on Audio Deepfake problems. <p> -->
			<!-- <p align="justify"> üí° Audio Deepfake: I have spent several years exploring Audio Deepfake problem I am an early researcher in Singing Voice Deepfake (SingFake) and also focus on CodecFake (e.g., CodecFake+). I am honored that our Source Tracing works on CodecFake received a Best Student Paper Nominee at IEEE ASRU 2025. </p> -->

		  <p style="text-align: justify; line-height: 1.6;">
			    I have focused on the <strong>audio deepfake problem</strong> for several years. 
			    I proposed <a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.html" target="_blank">SingGraph</a>, 
			    bridging <strong>music and speech understanding</strong> for <strong>SingFake</strong> detection. 
			    I also developed <a href="https://arxiv.org/abs/2501.08238" target="_blank">CodecFake+</a>, a large-scale dataset using 
			    <strong>neural codec resynthesis</strong> as a scalable proxy for TTS/VC data. 
			    Building on this, we introduced the <strong>source tracing</strong> task and proposed 
				<a href="https://arxiv.org/abs/2506.07294" target="_blank">SASTNet</a>, recognized as the 
				<strong>üèÜ Best Student Paper Nominee at IEEE ASRU 2025</strong>.
			</p>


		    <!-- <p style="text-align: justify; line-height: 1.6;">
				I have spent several years exploring the <strong>audio deepfake problem</strong>. 
				I proposed the <a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.html" target="_blank">SingGraph</a> model, 
				bridging <strong>music and speech understanding</strong> for <strong>SingFake</strong> detection.
				Furthermore, I developed <a href="https://arxiv.org/abs/2501.08238" target="_blank">CodecFake+</a>, a large-scale dataset leveraging 
				<strong>neural codec resynthesis data</strong> as a scalable proxy for expensive TTS/VC data.
				Based on this, we defined the <strong>source tracing</strong> task for CodecFake and proposed 
				<a href="https://arxiv.org/abs/2506.07294" target="_blank">SASTNet</a>. By effectively integrating 
				<strong>semantic-acoustic</strong> information to achieve better generalization, SASTNet was honored as a 
				<strong>üèÜ Best Student Paper Nominee at IEEE ASRU 2025</strong>.
			</p> -->
		    
		    <p style="text-align: justify; line-height: 1.6;">
		        <!-- In addition to my work in Audio Deepfake, I have been expanding my research scope into Audio/Text LLMs, contributing to projects: <a href="https://arxiv.org/abs/2507.02768" target="_blank">DeSTA2.5</a>, 
			    <a href="https://aclanthology.org/2024.findings-acl.616/" target="_blank">Codec-SUPERB</a>, and <a href="https://openreview.net/forum?id=s7lzZpAW7T" target="_blank">Dynamic-SUPERB Phase 2</a>. 
				Parallel to this, I also mentor junior researchers in RAG applications, our collaborative work "<a href="https://aclanthology.org/2025.rocling-main.6/" target="_blank">RAG for Historical Archives</a>" that recently earned the üèÜ ROCLING 2025 Best Paper Award.			 -->
				In addition to my work on audio deepfakes, I have expanded my research scope to Audio/Text LLMs, including contributions to <a href="https://arxiv.org/abs/2507.02768" target="_blank">DeSTA2.5</a>, 
				<a href="https://aclanthology.org/2024.findings-acl.616/" target="_blank">Codec-SUPERB</a>, and <a href="https://openreview.net/forum?id=s7lzZpAW7T" target="_blank">Dynamic-SUPERB Phase 2</a>. 
				Meanwhile, I mentor junior researchers in RAG applications, and our collaborative project, "<a href="https://aclanthology.org/2025.rocling-main.6/" target="_blank">RAG for Historical Archives</a>," was honored with the üèÜ ROCLING 2025 Best Paper Award.

			</p>
		  
		  <h2>Selected Publications <small style="font-size: 14px; font-weight: normal;">(* equal contribution)</small></h2>

			<div id="pub-container"></div>
			
			<!-- YAML parser -->
			<script src="https://cdn.jsdelivr.net/npm/js-yaml@4/dist/js-yaml.min.js"></script>
			
			<script>
			  function escapeHtml(str) {
			    // Âè™ escape title/venue ÈÄôÁ®ÆÁ¥îÊñáÂ≠óÊ¨Ñ‰ΩçÔºõauthors/links ÊàëÂÄëÊú¨‰æÜÂ∞±ÂÖÅË®±Â∞ëÈáè HTMLÔºà‰æãÂ¶Ç <u>Ôºâ
			    return String(str)
			      .replaceAll("&", "&amp;")
			      .replaceAll("<", "&lt;")
			      .replaceAll(">", "&gt;")
			      .replaceAll('"', "&quot;")
			      .replaceAll("'", "&#039;");
			  }
			
			  function renderPub(p) {
			    const authors = p.authors_short || p.authors_full || "";
			    const title = p.title ? escapeHtml(p.title) : "";
			    const venue = p.venue ? escapeHtml(p.venue) : "";
			
			    const linksHtml = (p.links || [])
			      .map(l => `<a href="${l.url}" target="_blank" rel="noopener noreferrer">${escapeHtml(l.name)}</a>`)
			      .join(" / ");
			
			    return `
			      <li class="pub-item">
			        <div class="pub-title"><b>${title}</b></div>
			        ${authors ? `<div class="pub-authors">${authors}</div>` : ""}
			        ${venue ? `<div class="pub-venue"><i>${venue}</i></div>` : ""}
			        ${linksHtml ? `<div class="pub-links">${linksHtml}</div>` : ""}
			      </li>
			    `;
			  }
			
			  fetch("pub.yml")
			    .then(res => res.text())
			    .then(text => {
			      const data = jsyaml.load(text); // ÈÄôË£°ÊúÉÂæóÂà∞‰∏ÄÂÄã array
			      const pubs = Array.isArray(data) ? data : [];
			      const html = `
			        <div class="pub-list">
			          <ol class="pub-ol" reversed>
			            ${pubs.map(renderPub).join("")}
			          </ol>
			        </div>
			      `;
			      document.getElementById("pub-container").innerHTML = html;
			    })
			    .catch(err => {
			      document.getElementById("pub-container").innerHTML =
			        `<p style="color:#b00;">Failed to load publications: ${escapeHtml(err)}</p>`;
			    });
			</script>
		  
		<!-- <div class="pub-list" style="padding-left: 20px;">
			<p align="justify">
			    	[11] Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, <u>Xuanjun Chen</u>, et al., <b>"DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment"</b> in arXiv 2025. 
				<br>
					<a href="https://arxiv.org/abs/2507.02768">arXiv</a> / 
				<a href="https://github.com/kehanlu/DeSTA2.5-Audio">Code</a>
			</p>
			  
			<p align="justify">	   
			    	[10] Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, <u>Xuanjun Chen</u>, et al., <b>"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"</b> in ICLR 2025.
				<br>
				<a href="https://openreview.net/forum?id=s7lzZpAW7T">OpenReview</a> / 
				<a href="https://arxiv.org/abs/2411.05361v1">arXiv</a> / 
				<a href="https://github.com/dynamic-superb/dynamic-superb">Code</a>
			</p>

			<p align="justify">
			    	[9] Claire Lin*, Bo-Han Feng*, <u>Xuanjun Chen*</u>, Te-Lun Yang, Hung-yi Lee, Jyh-Shing Roger Jang, <b>"A Preliminary Study of RAG for Taiwanese Historical Archives"</b> in ROCLING 2025 (üèÜ Best Paper Award). 
				<br>
					<a href="https://aclanthology.org/2025.rocling-main.6/">ACL Anthology</a> / 	
					<a href="https://arxiv.org/abs/2511.07445">arXiv</a>
			</p>
		</div>

		<p align="justify">	   
		    	[8] <u>Xuanjun Chen*</u>, Jiawei Du*, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee, <b>"CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset,"</b> Preprint, 2025.
			<br>
			<a href="https://arxiv.org/pdf/2501.08238v2">arXiv</a> / 
			<a href="https://responsiblegenai.github.io/CodecFake-Plus-Dataset/">Project Page</a> / 
			<a href="https://huggingface.co/datasets/CodecFake/CodecFake_Plus_Dataset">Hugging Face</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Plus-Dataset">Code</a>
		</p> -->
		  
		<!-- <p align="justify">
		    	[8] <u>Xuanjun Chen*</u>, Shih-Peng Cheng*, Jiawei Du, Lin Zhang, Xiaoxiao Miao, Chung-Che Wang, Haibin Wu, et al., <b>"Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling,"</b> in arxiv 2025. 
			<br>
			<a href="https://arxiv.org/abs/2508.02000">arXiv</a>
		</p> -->

		<!-- <p align="justify">
		    	[7] <u>Xuanjun Chen</u>, Chia-Yu Hu, I-Ming Lin, Yi-Cheng Lin, I-Hsiang Chiu, You Zhang, Sung-Feng Huang, Yi-Hsuan Yang, Haibin Wu, et al., <b>"How Does Instrumental Music Help SingFake Detection?"</b> in ICASSP 2026. 
			<br>
				<a href="https://arxiv.org/abs/2509.14675">arXiv</a>
		</p>
	      
	  	<p align="justify">
		    	[6] <u>Xuanjun Chen*</u>, I-Ming Lin*, Lin Zhang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Towards Generalized Source Tracing for Codec-Based Deepfake Speech,"</b> in IEEE ASRU 2025 (üèÜ Best Student Paper nominee). 
			<br>
		      	<a href="https://arxiv.org/abs/2506.07294">arXiv</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
		</p>
	      
		<p align="justify">
		    	[5] <u>Xuanjun Chen*</u>, I-Ming Lin*, Lin Zhang, Jiawei Du, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang. <b>"Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy,"</b> in INTERSPEECH 2025. 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2025/chen25j_interspeech.pdf">ISCA Archive</a> / 
			<a href="https://arxiv.org/pdf/2505.12994">arXiv</a> / 
			<a href="https://github.com/ResponsibleGenAI/CodecFake-Source-Tracing">Code</a>
		</p>

		    
		<p align="justify">
		    	[4] <u>Xuanjun Chen</u>, Haibin Wu, Jyh-Shing Roger Jang, and Hung-yi Lee. <b>"Singing Voice Graph Modeling for SingFake Detection,"</b> in INTERSPEECH 2024 (Oral). 
			<br>
			<a href="https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.pdf">ISCA Archive</a> /
			<a href="https://arxiv.org/pdf/2406.03111">arXiv</a> /
			<a href="https://github.com/xjchenGit/SingGraph.git">Code</a> /
			<a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Lightning Talk</a>
	    	</p> -->

			<!-- <p align="justify">	   
		    	[4] <u>Xuanjun Chen</u>, Haibin Wu, Chung-Che Wang, Hung-yi Lee, and Jyh-Shing Roger Jang, <b>"Multimodal Transformer Distillation for Audio-Visual Synchronization,"</b> in ICASSP 2024.
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10446372">IEEE Xplore</a> /
				<a href="https://arxiv.org/abs/2210.15563">arXiv</a> /
				<a href="https://github.com/xjchenGit/MTDVocaLiST">Code</a> /
				<a href="docs/MTDVocaLiST_poster.pdf">Poster</a>
	    	</p> -->

			<!-- <p align="justify">
				[3] Haibin Wu, <u>Xuanjun Chen</u>, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, and Hung-yi Lee. <b>"Towards audio language modeling-an overview,"</b> Overview Report, Feb. 2024.
				<br>
				<a href="https://arxiv.org/abs/2402.13236">arXiv</a> /
				<a href="https://github.com/ga642381/speech-trident">Awesome List</a>
			</p>
			    
			<p align="justify">
				[2] Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, <u>Xuanjun Chen</u>, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, and Hung-yi Lee. <b>"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models,"</b> in Findings of ACL 2024. 
				<br>
				<a href="https://aclanthology.org/2024.findings-acl.616/">ACL Anthology</a> /
				<a href="https://arxiv.org/abs/2402.13071">arXiv</a> /
				<a href="https://codecsuperb.com/">Leaderboard</a> /
				<a href="https://github.com/voidful/Codec-SUPERB">Code</a> /
				<a href="https://huggingface.co/Codec-SUPERB">Huggingface</a>
			</p>
	
			<p align="justify">
				[1] Haibin Wu, <u>Xuanjun Chen</u>, Yi-Cheng Lin, Jiawei Du, Kai-Wei Chang, Ke-Han Lu, Alexander Liu, Ho-Lam Chung, Yuan-Kuei Wu, Dongchao Yang, Songxiang Liu, Yi-Chiao Wu, Xu Tan, James Glass, Shinji Watanabe, and Hung-yi Lee. <b>"Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural codec models,"</b> in IEEE SLT 2024.
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10832364">IEEE Xplore</a> /
				<a href="https://arxiv.org/abs/2409.14085">arXiv</a>
			</p> -->

        <!-- <h2>Selected Honors</h2> -->
			<!-- <p align="justify">2025: National Science and Technology Council Conference Subsidy </p> -->
			<!-- <p align="justify">2024-25: (2x) NSTC Student Travel Grant, NSTC Taiwan </p> -->

		  
		<!-- <p align="justify">2025: CTCI Foundation Research Scholarship for Overseas Students</p>
	  	<p align="justify">2025: ACLCLP Student Travel Grant, ACLCLP Taiwan</p>
		<p align="justify">2024: Google Student Travel Grant, Google LLC </p>
		<p align="justify">2024: CTCI Foundation Bursary Award for Overseas Students</p>
		<p align="justify">2024-25: (2x) NSTC International Academic Conferences Subsidy </p>
		<p align="justify">2020-25: (5x) Kwong Tung Community Outstanding Student Scholarship </p>
		<p align="justify">2021: Ranked 3rd/42 teams in LA track of ASVspoof 2021 challenge </p>
		<p align="justify">2018-20: Certificate of Achievement, Taiwan Tech (Top 5%, 3 semesters)</p>
	  	<p align="justify">2017: SZIIT Academic Award (3rd Prize), SZIIT</p>
	    <p align="justify">2016: National Encouragement Scholarship, Ministry of Education</p> -->


		  
		<!-- <p align="justify">2017: National Bronze Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016-17: National Encouragement Scholarship; 3rd Prize, SZIIT Acad. Award </p> -->
	      	<!-- <p>2024: National Science and Technology Council (NSTC) Conference Subsidies, NSTC Taiwan </p> -->
		<!-- <p align="justify">2017: National Bronze and Guangdong Provincial Gold Awards, 3rd China College Internet Entrep. Comp. </p> -->
		<!-- <p align="justify">2016: National Encouragement Scholarship</p> -->
		<!-- <p align="justify">2017: 3rd Prize, SZIIT Academic Award</p>
			<p align="justify">2016: National Encouragement Scholarship</p> -->
	      <!-- <p align="justify">2016-2017: National Encouragement Scholarship & 3rd Prize, SZIIT Academic Award </p> -->

      	<!-- <h2>Selected Serivces</h2>
		<!-- <p align="justify">2023-Present: <b>Reviewer/Program Committee</b>, ACL ('24), EMNLP ('24), ICASSP ('23-'25), INTERSPEECH ('25), COLING ('24-'25), MLSP ('24), IALP ('24), ECCV AVGenL ('24)</p> -->
		<!-- <p align="justify">2023-Now: <b>Admin Assistant,</b> <a href="https://nvcenter.ntu.edu.tw/"> NVIDIA-NTU AI Joint Innovation Center</a>, NTU</p> -->
	      	<!-- <p align="justify">2025: <b>Co-Organizer,</b> <a href="https://codecfake.github.io/RespSA-GenAI/"> Responsible Speech & Audio Generative AI,</a> Special Session at IEEE ASRU 2025</p> -->
			<!-- <p align="justify">2024: <b>Technical Committee</b>, <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024: <b>Invited Speaker,</b> <a href="https://svddchallenge.org/challenges/special_session_ieee_slt.html">Topic: "Singing Voice Graph Modeling for SingFake Detection,"</a> SVDD Special Session at IEEE SLT 2024</p> -->
		<!-- <p align="justify">2024-25: <b>Teaching Assistant</b>, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php"> Intro. to Generative AI</a> & <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php">Machine Learning</a></p> -->
      		<!-- <p align="justify">2023-Now: <b>Reviewer:</b> AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI, IALP</p>  -->
		  <!-- , ECCV AVGenL -->
		  <!-- <br> -->

		  <h2>Selected Honors</h2>
			<ul>
			  <li>
			    2026: <b>NTU Mr. Wen Tzu-Hsiang Memorial Scholarship</b>, only 9 recipients in NTU.
			  </li>
				<li>
			    2025: <b>Best Student Paper Nominee</b>, The IEEE Autom. Speech Recognit. and Underst. Workshop.
			  </li>
			  <li>
			    2025: <b>Best Paper Award</b>, The 37th Conf. on Comput. Linguist. and Speech Processing.
			  </li>
			  <li>
			    2025: <b>CTCI Foundation Research Scholarship for Overseas Students</b>, awarded by CTCI Foundation 
			  </li>
			  <li>
			    2025: <b>ACLCLP Student Travel Grant</b>, granted by ACLCLP. 
			  </li>
			<li>
			    2024: <b>Google Student Travel Grant</b>, granted by Google LLC. 
			  </li>
			  <li>
			    2024: <b>CTCI Foundation Bursary Award for Overseas Students</b>, awarded by CTCI Foundation 
			  </li>
			  <li>
			    2021: Ranked <b>3rd/42 submissions worldwide</b> on logical access track of ASVspoof 2021 challenge
			  </li>
			  <li>
			    2020-2025: <b>Kwong Tung Community Outstanding Student Scholarship</b>, awarded five years.
			  </li>
			  <li>
			    2018-2020: <b>Certificate of Achievement</b>, Taiwan Tech (Top 5% of studnets; three semesters)
			  </li>
			  <li>
			    2016: <b>National Encouragement Scholarship</b>, Ministry of Education
			  </li>
			</ul>
		  
			  <h2>Selected Services</h2>
				<ul>
				  <li>
				    2025: <b>Co-Organizer</b>, 
				    <a href="https://codecfake.github.io/RespSA-GenAI/">Responsible Speech & Audio Generative AI</a>, 
				    Special Session at IEEE ASRU 2025
				  </li>
				
				  <li>
				    2024: <b>Technical Committee</b>, 
				    <a href="https://codecsuperb.github.io/">Codec-SUPERB Challenge</a>, 
				    Special Session at IEEE SLT 2024
				  </li>
				
				  <li>
				    2023‚ÄìNow: <b>Reviewer</b>: 
				    AAAI, ACL, EMNLP, ICASSP, INTERSPEECH, ASRU, COLING, MLSP, IJPRAI
				  </li>
				</ul>
		</section>
		<!-- <footer>
			<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cee3f2&w=200&t=tt&d=nEmSewDPXqNGs14be_z5YYcQJ2bmdcQr14eiveUCbnA&co=ffffff&cmn=005fa3&ct=2d78ad&cmo=980000'></script>
		</footer> -->
	</div>
	<script>
      fetch('header.html')
        .then(response => response.text())
        .then(data => {
          document.getElementById('sidebar').innerHTML = data;
          
          // --- 1. Ë®≠ÂÆö About me È´ò‰∫Æ ---
          const aboutLink = document.getElementById('nav-about');
          const pubLink = document.getElementById('nav-pub');
          if (aboutLink && pubLink) {
            // About È†ÅÈù¢ÔºåÊâÄ‰ª• About È´ò‰∫Æ
            aboutLink.style.textDecoration = "underline";
            aboutLink.style.textUnderlineOffset = "6px";
            aboutLink.style.color = "#000";
            aboutLink.style.fontWeight = "600";
            // Pub ÁÅ∞Êéâ
            pubLink.style.textDecoration = "none";
            pubLink.style.color = "#888";
            pubLink.style.fontWeight = "500";
          }

          // --- 2. Á∂ÅÂÆö Follow ÊåâÈàï‰∫ã‰ª∂ ---
          const followBtn = document.getElementById('follow-btn');
          const socialList = document.getElementById('social-list');
          if (followBtn && socialList) {
            followBtn.addEventListener('click', function(e) {
              e.stopPropagation();
              socialList.classList.toggle('show');
            });
          }
        })
        .catch(err => console.error('Error loading header:', err));

      window.addEventListener('click', function(event) {
        const socialList = document.getElementById('social-list');
        if (socialList && socialList.classList.contains('show')) {
          if (!event.target.matches('#follow-btn')) {
            socialList.classList.remove('show');
          }
        }
      });
    </script>
  </body>
</html>
